# Issue #167: ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£å¯¾å¿œ â€” å®Ÿè£…è¨ˆç”»æ›¸

> **ä½œæˆæ—¥**: 2026-02-19  
> **å¯¾è±¡Issue**: [#167 ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£å¯¾å¿œï¼ˆTeams/Zoomä¼šè­°ã®ç›¸æ‰‹éŸ³å£°éŒ²éŸ³ï¼‰](https://github.com/hidetoshihonda/airecorder/issues/167)  
> **å‰æãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [Issue167_ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£_åˆ†æãƒ¬ãƒ“ãƒ¥ãƒ¼.md](Issue167_ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£_åˆ†æãƒ¬ãƒ“ãƒ¥ãƒ¼.md)  
> **è¦‹ç©ã‚Š**: 22hï¼ˆç´„3æ—¥ï¼‰  
> **åˆ¤å®š**: GO âœ…

---

## ğŸ“‹ å®Ÿè£…æ¦‚è¦

Teams/Zoomç­‰ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ä¼šè­°ä¸­ã«ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ï¼ˆç›¸æ‰‹ã®å£°ï¼‰ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã€éŒ²éŸ³ãƒ»æ–‡å­—èµ·ã“ã—ãƒ»ç¿»è¨³ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚  
æ—¢å­˜ã® `sharedStream` è¨­è¨ˆã‚’æ´»ç”¨ã—ã€æ–°è¦ `useAudioSource` ãƒ•ãƒƒã‚¯ã§éŸ³å£°ã‚½ãƒ¼ã‚¹ã‚’ä¸€å…ƒç®¡ç†ã™ã‚‹ã€‚

---

## ğŸ—ï¸ å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

### æ–°è¦ä½œæˆ

| ãƒ•ã‚¡ã‚¤ãƒ« | èª¬æ˜ |
|---------|------|
| `web/src/lib/audioStreamAdapter.ts` | MediaStream â†’ Azure SDK PushAudioInputStream å¤‰æ›ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ |
| `web/src/hooks/useAudioSource.ts` | éŸ³å£°ã‚½ãƒ¼ã‚¹ç®¡ç†ãƒ•ãƒƒã‚¯ï¼ˆmic/system/bothï¼‰ |

### å¤‰æ›´

| ãƒ•ã‚¡ã‚¤ãƒ« | å¤‰æ›´å†…å®¹ |
|---------|---------|
| `web/src/hooks/useSpeechRecognition.ts` | `sharedStream` æ¸¡ã—æ™‚ã« `fromStreamInput` ã‚’ä½¿ç”¨ |
| `web/src/hooks/useTranslationRecognizer.ts` | `sharedStream` ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ  + `fromStreamInput` å¯¾å¿œ |
| `web/src/app/page.tsx` | éŸ³å£°ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰UIè¿½åŠ  + `useAudioSource` çµ±åˆ |
| `web/src/app/settings/page.tsx` | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã‚½ãƒ¼ã‚¹è¨­å®šè¿½åŠ  |
| `web/src/types/index.ts` | `UserSettings` ã« `defaultAudioSource` è¿½åŠ  |
| `api/src/models/recording.ts` | ã‚µãƒ¼ãƒãƒ¼å´ `UserSettings` ã« `defaultAudioSource` è¿½åŠ  |
| `web/messages/ja.json` | æ—¥æœ¬èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ  |
| `web/messages/en.json` | è‹±èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ  |
| `web/messages/es.json` | ã‚¹ãƒšã‚¤ãƒ³èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ  |

---

## ğŸ“ Step 1: `audioStreamAdapter.ts` â€” MediaStream â†’ PushStream å¤‰æ›

**ãƒ•ã‚¡ã‚¤ãƒ«**: `web/src/lib/audioStreamAdapter.ts`ï¼ˆæ–°è¦ï¼‰

### ç›®çš„

`MediaStream`ï¼ˆgetUserMedia / getDisplayMedia ãŒè¿”ã™ï¼‰ã‚’ Azure Speech SDK ã® `PushAudioInputStream` ã«å¤‰æ›ã™ã‚‹ã€‚  
Azure SDK ã¯ **16kHz / 16bit / mono PCM** ã‚’è¦æ±‚ã™ã‚‹ãŸã‚ã€Web Audio API ã§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ã‚’è¡Œã†ã€‚

### å®Ÿè£…ã‚³ãƒ¼ãƒ‰

```typescript
"use client";

import * as SpeechSDK from "microsoft-cognitiveservices-speech-sdk";

/**
 * Float32Array (-1.0 ~ 1.0) ã‚’ Int16Array (-32768 ~ 32767) ã«å¤‰æ›ã™ã‚‹
 */
function float32ToInt16(float32Array: Float32Array): Int16Array {
  const int16Array = new Int16Array(float32Array.length);
  for (let i = 0; i < float32Array.length; i++) {
    const s = Math.max(-1, Math.min(1, float32Array[i]));
    int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
  }
  return int16Array;
}

/**
 * MediaStream ã‚’ Azure Speech SDK ã® PushAudioInputStream ã«å¤‰æ›ã™ã‚‹ã€‚
 *
 * AudioContext ã‚’ sampleRate: 16000 ã§ä½œæˆã—ã€ScriptProcessorNode ã§
 * PCM ãƒ‡ãƒ¼ã‚¿ã‚’ PushAudioInputStream ã«æ›¸ãè¾¼ã‚€ã€‚
 *
 * @returns pushStream, cleanup é–¢æ•°ï¼ˆAudioContext ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç”¨ï¼‰
 */
export function createPushStreamFromMediaStream(
  mediaStream: MediaStream
): { pushStream: SpeechSDK.PushAudioInputStream; cleanup: () => void } {
  const format = SpeechSDK.AudioStreamFormat.getWaveFormatPCM(16000, 16, 1);
  const pushStream = SpeechSDK.AudioInputStream.createPushStream(format);

  // AudioContext ã‚’ 16kHz ã§ä½œæˆï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ãŒè‡ªå‹•ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
  const audioContext = new AudioContext({ sampleRate: 16000 });
  const source = audioContext.createMediaStreamSource(mediaStream);

  // ScriptProcessorNode: ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º 4096, å…¥åŠ›1ch, å‡ºåŠ›1ch
  const processor = audioContext.createScriptProcessorNode(4096, 1, 1);

  processor.onaudioprocess = (event: AudioProcessingEvent) => {
    const float32Data = event.inputBuffer.getChannelData(0);
    const int16Data = float32ToInt16(float32Data);
    pushStream.write(int16Data.buffer);
  };

  source.connect(processor);
  // ScriptProcessorNode ã¯ destination ã«æ¥ç¶šã—ãªã„ã¨å‹•ä½œã—ãªã„
  processor.connect(audioContext.destination);

  const cleanup = () => {
    processor.disconnect();
    source.disconnect();
    audioContext.close().catch(() => {});
    pushStream.close();
  };

  return { pushStream, cleanup };
}
```

### é‡è¦ãƒã‚¤ãƒ³ãƒˆ

- `AudioContext({ sampleRate: 16000 })` ã§ãƒ–ãƒ©ã‚¦ã‚¶ã«è‡ªå‹•ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã›ã‚‹
- `ScriptProcessorNode` ã¯ deprecated ã ãŒã€å…¨ãƒ–ãƒ©ã‚¦ã‚¶ã§å®‰å®šå‹•ä½œã™ã‚‹ã€‚å°†æ¥çš„ã« `AudioWorklet` ã«ç§»è¡Œ
- `cleanup()` ã§å…¨ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºå®Ÿã«è§£æ”¾ã™ã‚‹

---

## ğŸ“ Step 2: `useAudioSource` ãƒ•ãƒƒã‚¯ â€” éŸ³å£°ã‚½ãƒ¼ã‚¹ç®¡ç†

**ãƒ•ã‚¡ã‚¤ãƒ«**: `web/src/hooks/useAudioSource.ts`ï¼ˆæ–°è¦ï¼‰

### ç›®çš„

3ã¤ã®éŸ³å£°ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰ï¼ˆmic / system / bothï¼‰ã‚’çµ±ä¸€çš„ã«ç®¡ç†ã—ã€  
å˜ä¸€ã® `MediaStream` ã‚’è¿”å´ã™ã‚‹ã€‚

### å®Ÿè£…ã‚³ãƒ¼ãƒ‰

```typescript
"use client";

import { useCallback, useRef, useState } from "react";

export type AudioSourceMode = "mic" | "system" | "both";

interface UseAudioSourceReturn {
  /** çµ±åˆã•ã‚ŒãŸ MediaStreamï¼ˆéŒ²éŸ³ãƒ»éŸ³å£°èªè­˜ã«ä½¿ç”¨ï¼‰ */
  stream: MediaStream | null;
  /** ã‚¹ãƒˆãƒªãƒ¼ãƒ å–å¾—ä¸­ãƒ•ãƒ©ã‚° */
  isAcquiring: boolean;
  /** ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ */
  error: string | null;
  /** ã‚¹ãƒˆãƒªãƒ¼ãƒ å–å¾—ï¼ˆéŒ²éŸ³é–‹å§‹æ™‚ã«å‘¼ã¶ï¼‰ */
  acquireStream: () => Promise<MediaStream>;
  /** ã‚¹ãƒˆãƒªãƒ¼ãƒ è§£æ”¾ï¼ˆéŒ²éŸ³åœæ­¢æ™‚ã«å‘¼ã¶ï¼‰ */
  releaseStream: () => void;
  /** getDisplayMedia API ãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ */
  isSystemAudioSupported: boolean;
}

/**
 * éŸ³å£°ã‚½ãƒ¼ã‚¹ç®¡ç†ãƒ•ãƒƒã‚¯
 *
 * - 'mic':    getUserMedia({ audio }) â€” ãƒã‚¤ã‚¯ã®ã¿
 * - 'system': getDisplayMedia({ audio, systemAudio }) â€” ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã®ã¿
 * - 'both':   AudioContext ã§ mic + system ã‚’ãƒŸãƒƒã‚¯ã‚¹
 */
export function useAudioSource(mode: AudioSourceMode): UseAudioSourceReturn {
  const [stream, setStream] = useState<MediaStream | null>(null);
  const [isAcquiring, setIsAcquiring] = useState(false);
  const [error, setError] = useState<string | null>(null);

  // ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ç”¨ Ref
  const micStreamRef = useRef<MediaStream | null>(null);
  const systemStreamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const destinationRef = useRef<MediaStreamAudioDestinationNode | null>(null);

  // getDisplayMedia ã®åˆ©ç”¨å¯å¦åˆ¤å®š
  const isSystemAudioSupported =
    typeof navigator !== "undefined" &&
    !!navigator.mediaDevices &&
    "getDisplayMedia" in navigator.mediaDevices;

  /** ãƒã‚¤ã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å–å¾— */
  const getMicStream = useCallback(async (): Promise<MediaStream> => {
    return navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        sampleRate: 48000,
      },
    });
  }, []);

  /** ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å–å¾— */
  const getSystemStream = useCallback(async (): Promise<MediaStream> => {
    // @ts-expect-error â€” systemAudio ã¯ Chrome/Edge å›ºæœ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    const displayStream = await navigator.mediaDevices.getDisplayMedia({
      video: false, // æ˜ åƒä¸è¦ï¼ˆéŸ³å£°ã®ã¿ã‚­ãƒ£ãƒ—ãƒãƒ£ï¼‰
      audio: true,
      // Chrome/Edge: ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚’å«ã‚ã‚‹
      systemAudio: "include",
    });

    // video ãƒˆãƒ©ãƒƒã‚¯ã¯ä¸è¦ãªã®ã§å‰Šé™¤
    displayStream.getVideoTracks().forEach((track) => track.stop());

    // audio ãƒˆãƒ©ãƒƒã‚¯ãŒãªã„å ´åˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒéŸ³å£°å…±æœ‰ã‚’ãƒã‚§ãƒƒã‚¯ã—ãªã‹ã£ãŸï¼‰
    if (displayStream.getAudioTracks().length === 0) {
      throw new Error(
        "ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å…±æœ‰ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã§ã€Œã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚’å…±æœ‰ã€ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚"
      );
    }

    return displayStream;
  }, []);

  /** 2ã¤ã® MediaStream ã‚’ AudioContext ã§ãƒŸãƒƒã‚¯ã‚¹ã™ã‚‹ */
  const mixStreams = useCallback(
    (mic: MediaStream, system: MediaStream): MediaStream => {
      const ctx = new AudioContext();
      const destination = ctx.createMediaStreamDestination();

      const micSource = ctx.createMediaStreamSource(mic);
      const systemSource = ctx.createMediaStreamSource(system);

      micSource.connect(destination);
      systemSource.connect(destination);

      audioContextRef.current = ctx;
      destinationRef.current = destination;

      return destination.stream;
    },
    []
  );

  /** ã‚¹ãƒˆãƒªãƒ¼ãƒ å–å¾— */
  const acquireStream = useCallback(async (): Promise<MediaStream> => {
    setIsAcquiring(true);
    setError(null);

    try {
      let resultStream: MediaStream;

      switch (mode) {
        case "mic": {
          const mic = await getMicStream();
          micStreamRef.current = mic;
          resultStream = mic;
          break;
        }

        case "system": {
          if (!isSystemAudioSupported) {
            throw new Error(
              "ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚Chrome ã¾ãŸã¯ Edge ã‚’ãŠä½¿ã„ãã ã•ã„ã€‚"
            );
          }
          const system = await getSystemStream();
          systemStreamRef.current = system;
          resultStream = system;
          break;
        }

        case "both": {
          if (!isSystemAudioSupported) {
            throw new Error(
              "ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚Chrome ã¾ãŸã¯ Edge ã‚’ãŠä½¿ã„ãã ã•ã„ã€‚"
            );
          }
          // ãƒã‚¤ã‚¯ã¨ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚’ä¸¦è¡Œå–å¾—
          const [mic, system] = await Promise.all([
            getMicStream(),
            getSystemStream(),
          ]);
          micStreamRef.current = mic;
          systemStreamRef.current = system;

          // AudioContext ã§ãƒŸãƒƒã‚¯ã‚¹
          resultStream = mixStreams(mic, system);
          break;
        }
      }

      // track.onended ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œå…±æœ‰ã‚’åœæ­¢ã€ã—ãŸå ´åˆï¼‰
      resultStream.getAudioTracks().forEach((track) => {
        track.onended = () => {
          setError("éŸ³å£°å…±æœ‰ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ");
          releaseStream();
        };
      });

      setStream(resultStream);
      return resultStream;
    } catch (err) {
      const message =
        err instanceof Error ? err.message : "éŸ³å£°ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ";
      setError(message);
      throw err;
    } finally {
      setIsAcquiring(false);
    }
  }, [mode, getMicStream, getSystemStream, mixStreams, isSystemAudioSupported]);

  /** ã‚¹ãƒˆãƒªãƒ¼ãƒ è§£æ”¾ */
  const releaseStream = useCallback(() => {
    // AudioContext ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if (audioContextRef.current) {
      audioContextRef.current.close().catch(() => {});
      audioContextRef.current = null;
      destinationRef.current = null;
    }

    // ãƒã‚¤ã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒ åœæ­¢
    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((t) => t.stop());
      micStreamRef.current = null;
    }

    // ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ åœæ­¢
    if (systemStreamRef.current) {
      systemStreamRef.current.getTracks().forEach((t) => t.stop());
      systemStreamRef.current = null;
    }

    setStream(null);
    setError(null);
  }, []);

  return {
    stream,
    isAcquiring,
    error,
    acquireStream,
    releaseStream,
    isSystemAudioSupported,
  };
}
```

### ãƒªã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«

```
acquireStream()
  â”œâ”€â”€ getUserMedia â†’ micStreamRef
  â”œâ”€â”€ getDisplayMedia â†’ systemStreamRef
  â””â”€â”€ AudioContext.mixStreams â†’ audioContextRef + destinationRef
      â””â”€â”€ stream (çµ±åˆ MediaStream)

releaseStream()
  â”œâ”€â”€ audioContext.close()
  â”œâ”€â”€ micStream.getTracks().stop()
  â””â”€â”€ systemStream.getTracks().stop()
```

---

## ğŸ“ Step 3: `useSpeechRecognition` ã® `fromStreamInput` å¯¾å¿œ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `web/src/hooks/useSpeechRecognition.ts`ï¼ˆå¤‰æ›´ï¼‰

### å¤‰æ›´ç®‡æ‰€

#### 3.1 ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¿½åŠ 

```typescript
import { createPushStreamFromMediaStream } from "@/lib/audioStreamAdapter";
```

#### 3.2 `startConversationTranscriber` å†…ã® AudioConfig åˆ†å²ï¼ˆL61ä»˜è¿‘ï¼‰

```typescript
// Before:
const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();

// After:
let audioConfig: SpeechSDK.AudioConfig;
let pushStreamCleanup: (() => void) | null = null;

if (sharedStream) {
  const { pushStream, cleanup } = createPushStreamFromMediaStream(sharedStream);
  audioConfig = SpeechSDK.AudioConfig.fromStreamInput(pushStream);
  pushStreamCleanup = cleanup;
} else {
  audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
}
```

#### 3.3 `startListening` å†…ã® AudioConfig åˆ†å²ï¼ˆL158ä»˜è¿‘ï¼‰

åŒæ§˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ `fromStreamInput` å¯¾å¿œã€‚

#### 3.4 ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è¿½åŠ 

`stopListening` ã¨ `pauseListening` ã§ `pushStreamCleanup?.()` ã‚’å‘¼ã¶ã€‚  
â†’ `pushStreamCleanupRef` ã‚’ `useRef` ã§ç®¡ç†ã™ã‚‹ã€‚

### æ³¨æ„: ConversationTranscriber ã¨ã®äº’æ›æ€§

`ConversationTranscriber` ãŒ `fromStreamInput` ã‚’å—ã‘ä»˜ã‘ã‚‹ã‹ã¯ **è¦æ¤œè¨¼**ã€‚  
å—ã‘ä»˜ã‘ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯:

```typescript
if (enableSpeakerDiarization && sharedStream) {
  // âš ï¸ ConversationTranscriber ã¯ fromStreamInput æœªå¯¾å¿œã®å¯èƒ½æ€§
  // â†’ ãƒã‚¤ã‚¯ãƒ¢ãƒ¼ãƒ‰ã‚’å¼·åˆ¶ or ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
  setError("è©±è€…è­˜åˆ¥ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚è©±è€…è­˜åˆ¥ã‚’OFFã«ã™ã‚‹ã‹ã€ãƒã‚¤ã‚¯ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„ã€‚");
  return;
}
```

---

## ğŸ“ Step 4: `useTranslationRecognizer` ã® `sharedStream` å¯¾å¿œ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `web/src/hooks/useTranslationRecognizer.ts`ï¼ˆå¤‰æ›´ï¼‰

### å¤‰æ›´ç®‡æ‰€

#### 4.1 Options ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã« `sharedStream` è¿½åŠ 

```typescript
interface UseTranslationRecognizerOptions {
  subscriptionKey: string;
  region: string;
  sourceLanguage: string;
  targetLanguage: string;
  phraseList?: string[];
  sharedStream?: MediaStream | null;  // â˜… è¿½åŠ 
}
```

#### 4.2 `startListening` å†…ã® AudioConfig åˆ†å²ï¼ˆL115ä»˜è¿‘ï¼‰

```typescript
// Before:
const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();

// After:
let audioConfig: SpeechSDK.AudioConfig;
let pushStreamCleanup: (() => void) | null = null;

if (sharedStream) {
  const { pushStream, cleanup } = createPushStreamFromMediaStream(sharedStream);
  audioConfig = SpeechSDK.AudioConfig.fromStreamInput(pushStream);
  pushStreamCleanup = cleanup;
  pushStreamCleanupRef.current = cleanup;
} else {
  audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
}
```

#### 4.3 ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è¿½åŠ 

`stopListening` ã§ `pushStreamCleanupRef.current?.()` ã‚’å‘¼ã¶ã€‚

---

## ğŸ“ Step 5: `UserSettings` å‹æ‹¡å¼µ

### 5.1 ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ â€” `web/src/types/index.ts`

```typescript
export interface UserSettings {
  // ... æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ ...
  defaultAudioSource?: AudioSourceMode;  // â˜… è¿½åŠ  ('mic' | 'system' | 'both')
}
```

### 5.2 ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ â€” `api/src/models/recording.ts`

```typescript
export interface UserSettings {
  // ... æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ ...
  defaultAudioSource?: "mic" | "system" | "both";  // â˜… è¿½åŠ 
}
```

### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

- `defaultAudioSource` ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ `'mic'`ï¼ˆæ—¢å­˜å‹•ä½œã¨å®Œå…¨äº’æ›ï¼‰
- æœªè¨­å®šæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: `settings.defaultAudioSource ?? 'mic'`

---

## ğŸ“ Step 6: `page.tsx` â€” éŸ³å£°ã‚½ãƒ¼ã‚¹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ UI

**ãƒ•ã‚¡ã‚¤ãƒ«**: `web/src/app/page.tsx`ï¼ˆå¤‰æ›´ï¼‰

### 6.1 State è¿½åŠ 

```typescript
const [audioSourceMode, setAudioSourceMode] = useState<AudioSourceMode>(
  settings.defaultAudioSource ?? "mic"
);
```

### 6.2 `useAudioSource` ãƒ•ãƒƒã‚¯çµ±åˆ

```typescript
import { useAudioSource, AudioSourceMode } from "@/hooks/useAudioSource";

const {
  stream: sharedStream,
  isAcquiring: isAcquiringAudio,
  error: audioSourceError,
  acquireStream,
  releaseStream,
  isSystemAudioSupported,
} = useAudioSource(audioSourceMode);
```

### 6.3 ãƒ•ãƒƒã‚¯å‘¼ã³å‡ºã—å¤‰æ›´

```typescript
// useSpeechRecognition ã« sharedStream ã‚’æ¸¡ã™
const { ... } = useSpeechRecognition({
  subscriptionKey: speechConfig.subscriptionKey,
  region: speechConfig.region,
  language: sourceLanguage,
  enableSpeakerDiarization,
  phraseList: settings.phraseList ?? [],
  sharedStream,  // â˜… è¿½åŠ 
});

// useTranslationRecognizer ã« sharedStream ã‚’æ¸¡ã™
const { ... } = useTranslationRecognizer({
  subscriptionKey: speechConfig.subscriptionKey,
  region: speechConfig.region,
  sourceLanguage,
  targetLanguage,
  phraseList: settings.phraseList ?? [],
  sharedStream,  // â˜… è¿½åŠ 
});

// useAudioRecorder ã« sharedStream ã‚’æ¸¡ã™
const { ... } = useAudioRecorder({
  audioQuality: settings.audioQuality,
  sharedStream,  // â˜… è¿½åŠ ï¼ˆæ—¢å­˜ã® sharedStream å¯¾å¿œã‚’æ´»ç”¨ï¼‰
});
```

### 6.4 `handleStartRecording` å¤‰æ›´

```typescript
const handleStartRecording = async () => {
  if (!requireAuth(t("startRecording"))) return;
  if (!canStart) return;

  dispatch({ type: "START" });
  // ... æ—¢å­˜ã®ãƒªã‚»ãƒƒãƒˆå‡¦ç† ...

  try {
    // â˜… ã¾ãšã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å–å¾—
    const audioStream = await acquireStream();

    // ã‚¹ãƒˆãƒªãƒ¼ãƒ å–å¾—å¾Œã«éŸ³å£°èªè­˜ãƒ»éŒ²éŸ³ã‚’é–‹å§‹
    startListening();
    await startAudioRecording(audioStream);  // sharedStream çµŒç”±ã§è‡ªå‹•çš„ã«ä½¿ç”¨
    dispatch({ type: "START_SUCCESS" });
  } catch (err) {
    const message = err instanceof Error ? err.message : t("startRecordingFailed");
    dispatch({ type: "START_FAILURE", error: message });
    releaseStream();
  }
};
```

### 6.5 `handleStopRecording` å¤‰æ›´

```typescript
const handleStopRecording = () => {
  if (!canStop) return;
  dispatch({ type: "STOP" });
  stopListening();
  stopAudioRecording();
  releaseStream();  // â˜… ã‚¹ãƒˆãƒªãƒ¼ãƒ è§£æ”¾è¿½åŠ 
  dispatch({ type: "STOP_SUCCESS" });
};
```

### 6.6 UI â€” éŸ³å£°ã‚½ãƒ¼ã‚¹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼

éŒ²éŸ³ãƒœã‚¿ãƒ³ã®å·¦å´ã€è¨€èªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã®å¾Œã«é…ç½®:

```tsx
{/* Audio Source Mode Selector */}
{isSystemAudioSupported && (
  <>
    <div className="h-6 w-px bg-gray-200" />
    <Select
      value={audioSourceMode}
      onValueChange={(v) => setAudioSourceMode(v as AudioSourceMode)}
    >
      <SelectTrigger className="h-8 w-44 text-xs">
        <SelectValue />
      </SelectTrigger>
      <SelectContent>
        <SelectItem value="mic">
          ğŸ¤ {t("audioSourceMic")}
        </SelectItem>
        <SelectItem value="system">
          ğŸ–¥ï¸ {t("audioSourceSystem")}
        </SelectItem>
        <SelectItem value="both">
          ğŸ¤+ğŸ–¥ï¸ {t("audioSourceBoth")}
        </SelectItem>
      </SelectContent>
    </Select>
  </>
)}
```

### 6.7 è©±è€…è­˜åˆ¥ã¨ã®æ’ä»–åˆ¶å¾¡

```typescript
// è©±è€…è­˜åˆ¥ ON + ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ãƒ¢ãƒ¼ãƒ‰ â†’ è­¦å‘Šè¡¨ç¤º
{enableSpeakerDiarization && audioSourceMode !== "mic" && (
  <div className="text-xs text-amber-600 bg-amber-50 rounded px-2 py-1">
    âš ï¸ {t("speakerDiarizationSystemAudioWarning")}
  </div>
)}
```

---

## ğŸ“ Step 7: è¨­å®šç”»é¢ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã‚½ãƒ¼ã‚¹è¨­å®š

**ãƒ•ã‚¡ã‚¤ãƒ«**: `web/src/app/settings/page.tsx`ï¼ˆå¤‰æ›´ï¼‰

éŸ³å£°è¨­å®šã‚«ãƒ¼ãƒ‰ã«ã€Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã‚½ãƒ¼ã‚¹ã€ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‚’è¿½åŠ :

```tsx
{/* Default Audio Source */}
{isSystemAudioSupported && (
  <div className="flex items-center justify-between">
    <div>
      <label className="text-sm font-medium">{t("defaultAudioSource")}</label>
      <p className="text-xs text-gray-500">{t("defaultAudioSourceDescription")}</p>
    </div>
    <Select
      value={settings.defaultAudioSource ?? "mic"}
      onValueChange={(v) => updateSettings({ defaultAudioSource: v as AudioSourceMode })}
    >
      <SelectTrigger className="w-48">
        <SelectValue />
      </SelectTrigger>
      <SelectContent>
        <SelectItem value="mic">ğŸ¤ {t("audioSourceMic")}</SelectItem>
        <SelectItem value="system">ğŸ–¥ï¸ {t("audioSourceSystem")}</SelectItem>
        <SelectItem value="both">ğŸ¤+ğŸ–¥ï¸ {t("audioSourceBoth")}</SelectItem>
      </SelectContent>
    </Select>
  </div>
)}
```

---

## ğŸ“ Step 8: å¤šè¨€èªå¯¾å¿œï¼ˆi18n ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰

### `web/messages/ja.json` ã«è¿½åŠ 

```json
{
  "HomePage": {
    "audioSourceMic": "ãƒã‚¤ã‚¯ã®ã¿",
    "audioSourceSystem": "ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°",
    "audioSourceBoth": "ãƒã‚¤ã‚¯ï¼‹ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°",
    "speakerDiarizationSystemAudioWarning": "è©±è€…è­˜åˆ¥ã¯ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ãƒ¢ãƒ¼ãƒ‰ã§ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“",
    "systemAudioNotSupported": "ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚­ãƒ£ãƒ—ãƒãƒ£ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“",
    "audioShareStopped": "éŸ³å£°å…±æœ‰ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸ"
  },
  "Settings": {
    "defaultAudioSource": "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã‚½ãƒ¼ã‚¹",
    "defaultAudioSourceDescription": "éŒ²éŸ³é–‹å§‹æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰"
  }
}
```

### `web/messages/en.json` ã«è¿½åŠ 

```json
{
  "HomePage": {
    "audioSourceMic": "Microphone only",
    "audioSourceSystem": "System audio",
    "audioSourceBoth": "Mic + System audio",
    "speakerDiarizationSystemAudioWarning": "Speaker diarization is not available with system audio mode",
    "systemAudioNotSupported": "System audio capture is not supported in this browser",
    "audioShareStopped": "Audio sharing was stopped"
  },
  "Settings": {
    "defaultAudioSource": "Default audio source",
    "defaultAudioSourceDescription": "Default audio input mode when starting recording"
  }
}
```

### `web/messages/es.json` ã«è¿½åŠ 

```json
{
  "HomePage": {
    "audioSourceMic": "Solo micrÃ³fono",
    "audioSourceSystem": "Audio del sistema",
    "audioSourceBoth": "Mic + Audio del sistema",
    "speakerDiarizationSystemAudioWarning": "La identificaciÃ³n del hablante no estÃ¡ disponible con el modo de audio del sistema",
    "systemAudioNotSupported": "La captura de audio del sistema no es compatible con este navegador",
    "audioShareStopped": "Se detuvo el uso compartido de audio"
  },
  "Settings": {
    "defaultAudioSource": "Fuente de audio predeterminada",
    "defaultAudioSourceDescription": "Modo de entrada de audio predeterminado al iniciar la grabaciÃ³n"
  }
}
```

---

## ğŸ“ Step 9: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–

### 9.1 `page.tsx` â€” ã‚¨ãƒ©ãƒ¼é…åˆ—ã« audioSourceError ã‚’è¿½åŠ 

```typescript
const errors = [
  speechError,
  translationError,
  ttsError,
  audioError,
  fsmError,
  correctionError,
  audioSourceError,  // â˜… è¿½åŠ 
].filter(Boolean) as string[];
```

### 9.2 `useAudioSource` â€” å…±æœ‰åœæ­¢æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯

```typescript
// page.tsx ã§ onShareStopped ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨
const handleAudioShareStopped = useCallback(() => {
  // éŒ²éŸ³ã‚’åœæ­¢
  handleStopRecording();
}, [handleStopRecording]);

// useAudioSource ã«æ¸¡ã™ï¼ˆonEnded ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
```

---

## ğŸ“ Step 10: ãƒ†ã‚¹ãƒˆå®Ÿæ–½

### æ‰‹å‹•ãƒ†ã‚¹ãƒˆãƒãƒˆãƒªã‚¯ã‚¹

| # | ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ | ç’°å¢ƒ | æ‰‹é † | æœŸå¾…çµæœ |
|---|------------|------|------|---------|
| 1 | ãƒã‚¤ã‚¯ãƒ¢ãƒ¼ãƒ‰ã§éŒ²éŸ³ | Chrome | ã€Œãƒã‚¤ã‚¯ã®ã¿ã€ã‚’é¸æŠ â†’ éŒ²éŸ³é–‹å§‹ â†’ åœæ­¢ | æ—¢å­˜å‹•ä½œã¨åŒä¸€ã€‚å›å¸°ãªã— |
| 2 | ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã§éŒ²éŸ³ | Chrome | ã€Œã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã€ã‚’é¸æŠ â†’ éŒ²éŸ³é–‹å§‹ â†’ å…±æœ‰ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã§ã‚¿ãƒ–é¸æŠ â†’ åœæ­¢ | ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ãŒæ–‡å­—èµ·ã“ã—ã•ã‚Œã‚‹ |
| 3 | ä¸¡æ–¹ã§éŒ²éŸ³ | Chrome | ã€Œãƒã‚¤ã‚¯+ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã€â†’ éŒ²éŸ³é–‹å§‹ â†’ åœæ­¢ | ä¸¡æ–¹ã®éŸ³å£°ãŒæ–‡å­—èµ·ã“ã—ã•ã‚Œã‚‹ |
| 4 | å…±æœ‰åœæ­¢ | Chrome | éŒ²éŸ³ä¸­ã«ãƒ–ãƒ©ã‚¦ã‚¶ã®ã€Œå…±æœ‰ã‚’åœæ­¢ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ | éŒ²éŸ³ãŒåœæ­¢ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º |
| 5 | éå¯¾å¿œãƒ–ãƒ©ã‚¦ã‚¶ | Firefox | ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œãªã„ã“ã¨ã‚’ç¢ºèª | ã€Œã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã€é¸æŠè‚¢ãŒéè¡¨ç¤º |
| 6 | è©±è€…è­˜åˆ¥+ã‚·ã‚¹ãƒ†ãƒ éŸ³å£° | Chrome | è©±è€…è­˜åˆ¥ON â†’ ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚’é¸æŠ | è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º |
| 7 | ç¿»è¨³+ã‚·ã‚¹ãƒ†ãƒ éŸ³å£° | Chrome | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç¿»è¨³ON â†’ ã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã§éŒ²éŸ³ | ç¿»è¨³ãŒæ­£å¸¸å‹•ä½œ |
| 8 | Safari ã‚¿ãƒ–éŸ³å£° | Safari | ã€Œã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã€â†’ ã‚¿ãƒ–é¸æŠ | ã‚¿ãƒ–éŸ³å£°ã®ã¿ã‚­ãƒ£ãƒ—ãƒãƒ£ |
| 9 | iOS | Safari/iOS | ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œãªã„ã“ã¨ã‚’ç¢ºèª | éè¡¨ç¤º |
| 10 | è¨­å®šã®æ°¸ç¶šåŒ– | Chrome | è¨­å®šç”»é¢ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã‚½ãƒ¼ã‚¹ã‚’å¤‰æ›´ â†’ ãƒšãƒ¼ã‚¸å†èª­ã¿è¾¼ã¿ | è¨­å®šãŒä¿æŒã•ã‚Œã‚‹ |

---

## âš ï¸ æ—¢çŸ¥ã®åˆ¶ç´„ãƒ»æ³¨æ„äº‹é …

| # | åˆ¶ç´„ | å¯¾å‡¦ |
|---|------|------|
| 1 | `ScriptProcessorNode` ã¯ deprecated | ç¾çŠ¶ã¯å®‰å®šå‹•ä½œã€‚å°†æ¥çš„ã« `AudioWorklet` ã«ç§»è¡Œ |
| 2 | `ConversationTranscriber` (è©±è€…è­˜åˆ¥) ã¨ `fromStreamInput` ã®äº’æ›æ€§æœªæ¤œè¨¼ | è©±è€…è­˜åˆ¥ONæ™‚ã¯ãƒã‚¤ã‚¯ãƒ¢ãƒ¼ãƒ‰ã«åˆ¶é™ or ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ |
| 3 | Chrome ã® `getDisplayMedia` ã¯å¿…ãšãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œãŒå¿…è¦ï¼ˆAPI å‘¼ã³å‡ºã—ã ã‘ã§ã¯ä¸å¯ï¼‰ | éŒ²éŸ³ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯ãƒãƒ³ãƒ‰ãƒ©å†…ã§å‘¼ã¶ |
| 4 | `systemAudio: 'include'` ã¯ Chrome/Edge å›ºæœ‰ | TypeScript ã®å‹å®šç¾©ã«ãªã„ãŸã‚ `@ts-expect-error` ãŒå¿…è¦ |
| 5 | `video: false` ã§ã‚‚ Chrome ã¯ã‚¿ãƒ–å…±æœ‰ UI ã‚’è¡¨ç¤ºã™ã‚‹å ´åˆãŒã‚ã‚‹ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ åƒå…±æœ‰ã‚’é¸ã‚“ã§ã‚‚ video ãƒˆãƒ©ãƒƒã‚¯ã¯å³åº§ã« stop() |

---

## ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥

```
acquireStream() å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯:
  â”œâ”€â”€ mode: 'system' â†’ ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºã€Œã‚·ã‚¹ãƒ†ãƒ éŸ³å£°ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€
  â”‚                     â†’ è‡ªå‹•çš„ã« 'mic' ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ãªã„ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¤æ–­ï¼‰
  â”œâ”€â”€ mode: 'both'   â†’ Promise.all ã®ç‰‡æ–¹ãŒå¤±æ•—
  â”‚                     â†’ å–å¾—ã§ããŸæ–¹ã®ã¿ã§ç¶šè¡Œ or ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
  â””â”€â”€ mode: 'mic'    â†’ æ—¢å­˜ã® getUserMedia ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨åŒä¸€

éŒ²éŸ³ä¸­ã®å…±æœ‰åœæ­¢:
  â””â”€â”€ track.onended â†’ handleStopRecording() å‘¼ã³å‡ºã— + ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
```

---

## ğŸ“Š å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| Day | Step | ä½œæ¥­å†…å®¹ |
|-----|------|---------|
| Day 1 AM | Step 1-2 | `audioStreamAdapter.ts` + `useAudioSource.ts` æ–°è¦ä½œæˆ |
| Day 1 PM | Step 3-4 | `useSpeechRecognition` + `useTranslationRecognizer` ã® `fromStreamInput` å¯¾å¿œ |
| Day 2 AM | Step 5-6 | å‹å®šç¾© + `page.tsx` UI çµ±åˆ |
| Day 2 PM | Step 7-8 | è¨­å®šç”»é¢ + i18n |
| Day 3 AM | Step 9 | ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ– |
| Day 3 PM | Step 10 | æ‰‹å‹•ãƒ†ã‚¹ãƒˆï¼ˆChrome/Edge/Safari/Firefoxï¼‰ |

---

## ğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †

1. ãƒ–ãƒ©ãƒ³ãƒ `feat/system-audio-capture` ã‚’ä½œæˆ
2. Step 1ã€œ9 ã‚’å®Ÿè£…
3. ESLint / TypeScript ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆ
4. PR ã‚’ä½œæˆï¼ˆ`main` â† `feat/system-audio-capture`ï¼‰
5. CI é€šéç¢ºèª
6. ãƒãƒ¼ã‚¸ï¼‹è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤

---
