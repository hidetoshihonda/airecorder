# 🎯 声紋ベース話者識別（Speaker Diarization）実装計画書

**作成日**: 2026年2月6日  
**バージョン**: 2.0  
**ステータス**: 計画中（レビュー済み）

---

## ⚠️ 超一流エンジニアによる批判的レビュー

### 🚨 致命的リスク・ブロッカー

#### 1. **ConversationTranscriber に一時停止/再開APIが存在しない**

| 問題 | 詳細 |
|------|------|
| **現状** | `ConversationTranscriber` は `startTranscribingAsync()` と `stopTranscribingAsync()` のみ |
| **影響** | 計画書の「一時停止/再開機能」は**実装不可能** |
| **対策** | 停止→再開時に新しいインスタンスを作成する必要あり（話者IDがリセットされる可能性大） |

```typescript
// ❌ これは存在しない
transcriber.pauseTranscribingAsync();
transcriber.resumeTranscribingAsync();

// ✅ 実際にできること
transcriber.stopTranscribingAsync();  // 停止
// 再開するには新しいインスタンスが必要
const newTranscriber = new ConversationTranscriber(config, audioConfig);
newTranscriber.startTranscribingAsync();  // 話者IDがリセットされる
```

**🔴 対応必須**: TICKET-060のタスクから「一時停止/再開機能」を削除するか、制限事項として明記

---

#### 2. **話者IDの永続性がない**

| 問題 | 詳細 |
|------|------|
| **現状** | `speakerId` は "Guest-1", "Guest-2" という一時的なID |
| **影響** | セッション間で同じ人が同じIDになる保証がない |
| **例** | 今日の「Guest-1」が明日も「Guest-1」とは限らない |

**🔴 対応必須**: 
- TICKET-061の「次回起動時に話者名が復元される」は**話者マッピング**のみ復元可能（声での再識別は不可）
- Voice Profile（TICKET-064）を「中」から「高」に優先度変更を検討

---

#### 3. **中間結果の話者IDが "Unknown" になる問題**

公式ドキュメントより：
> "You might see `Speaker ID=Unknown` in some of the early intermediate results when the speaker isn't yet identified."

| 問題 | 詳細 |
|------|------|
| **現状** | 発話開始直後は `speakerId = "Unknown"` が返る |
| **影響** | リアルタイム表示で「現在の話者」が不明になる瞬間がある |
| **対策** | `SpeechServiceResponse_DiarizeIntermediateResults` を `true` に設定しても完全には解決しない |

**🟡 対応推奨**: UI設計で「話者識別中...」の状態を追加

---

### 🟠 重要な考慮事項

#### 4. **単一マイク入力の制限**

| 問題 | 詳細 |
|------|------|
| **現状** | `AudioConfig.fromDefaultMicrophoneInput()` は単一マイク |
| **影響** | 複数人が同じマイクに向かって話す必要がある |
| **制限** | 遠くの人の声は認識精度が低下、ノイズの影響大 |

**🟡 対応推奨**: 
- 「推奨マイク配置」のガイダンスをUIに追加
- マルチマイク入力は Web Audio API でミキシングが必要（追加工数）

---

#### 5. **翻訳機能との同時使用不可**

| 問題 | 詳細 |
|------|------|
| **現状** | `ConversationTranscriber` は翻訳機能を持たない |
| **影響** | 現在の翻訳機能（`TranslationRecognizer`）との同時使用が**不可能** |
| **対策** | 認識結果を別途 Translator API に送信する2段階処理が必要 |

**🔴 対応必須**: 
- 既存の `useSpeechRecognition` + `useTranslation` の組み合わせ方式を継続
- 話者識別結果を取得後、`translate()` を呼び出すフローに変更

```typescript
// 新しいフロー
transcriber.transcribed = async (s, e) => {
  const text = e.result.text;
  const speakerId = e.result.speakerId;
  
  // 翻訳は別途API呼び出し
  const translated = await translate(text, fromLang, toLang);
  
  addSegment({ text, speakerId, translated });
};
```

---

#### 6. **日本語リージョンの動作確認が必要**

| 問題 | 詳細 |
|------|------|
| **現状** | Japan East での Conversation Transcription のサポート状況が不明確 |
| **リスク** | 本番デプロイ後に動作しない可能性 |

**🔴 対応必須**: 
- 実装開始前に Japan East での動作確認を行う
- フォールバックリージョン（East US など）の設定を準備

---

#### 7. **料金体系の確認不足**

| 問題 | 詳細 |
|------|------|
| **現状** | Conversation Transcription の課金は通常の STT と異なる可能性 |
| **リスク** | 想定外のコスト発生 |

**🟡 対応推奨**: Azure 料金ページで Conversation Transcription の料金を確認

---

### 🟢 軽微な改善点

#### 8. **エラーハンドリングの不足**

計画書に記載されていない重要なエラーケース：

| エラーケース | 対応 |
|-------------|------|
| マイク権限拒否 | ユーザーフレンドリーなエラーメッセージ |
| ネットワーク切断 | 自動再接続 or 明示的なエラー表示 |
| Azure API制限超過 | リトライロジック |
| セッションタイムアウト | 自動再開 or 通知 |

---

#### 9. **テスト計画の現実性**

| 問題 | 詳細 |
|------|------|
| **現状** | テストコードが `playTestAudio()` を使用 |
| **問題** | ブラウザでは音声ファイル再生をマイク入力として使えない |
| **対策** | `AudioConfig.fromWavFileInput()` を使用するか、手動テストに切り替え |

---

#### 10. **既存機能との互換性リスク**

| 影響を受ける機能 | 確認事項 |
|----------------|---------|
| リアルタイム翻訳 | 翻訳タイミングの変更が必要 |
| TTS読み上げ | 話者切り替え時のTTS制御 |
| Blob Storage保存 | 話者情報を含む形式への変更 |
| AI議事録生成 | 話者情報を含むプロンプト設計 |

---

## 📋 修正版チケット依存関係

```
┌─────────────────────────────────────────────────────────────┐
│  TICKET-060-PRE: 事前検証（Japan East動作確認、料金確認）    │
│  ⏱️ 2時間 | 🔴 最高                                        │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  TICKET-060: ConversationTranscriber 基本実装               │
│  ⏱️ 4時間 | 🔴 最高                                        │
│  ⚠️ 一時停止/再開は「停止→再作成」で代替                    │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  TICKET-060-B: 翻訳機能統合                                 │
│  ⏱️ 2時間 | 🔴 最高                                        │
│  話者識別結果 → Translator API → 翻訳結果                   │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  TICKET-061: 話者管理・ラベリング                           │
│  ⏱️ 3時間 | 🔴 最高                                        │
│  ⚠️ セッション間の話者永続性なし（UIで明示）                │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  TICKET-062: 話者分離 UI                                    │
│  ⏱️ 4時間 | 🔴 最高                                        │
│  ⚠️ 「話者識別中...」状態の追加                             │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  TICKET-063: 既存フック統合                                 │
│  ⏱️ 4時間 | 🟡 高（工数増加: +1h）                          │
│  翻訳・TTS・保存機能との互換性確保                          │
└─────────────────────────────────────────────────────────────┘
```

---

## 📊 修正版工数見積もり

| チケット | タイトル | 当初工数 | 修正工数 | 差分 |
|---------|---------|---------|---------|------|
| TICKET-060-PRE | 事前検証 | - | 2h | **+2h** |
| TICKET-060 | 基本実装 | 4h | 4h | ±0 |
| TICKET-060-B | 翻訳統合 | - | 2h | **+2h** |
| TICKET-061 | 話者管理 | 3h | 3h | ±0 |
| TICKET-062 | UI | 4h | 4h | ±0 |
| TICKET-063 | 統合 | 3h | 4h | **+1h** |

### 合計工数（修正後）

| 範囲 | 当初 | 修正後 | 差分 |
|------|------|--------|------|
| **MVP** | 14h | **19h** | +5h |
| フル実装 | 22h | **27h** | +5h |

---

## ✅ 実装前チェックリスト

### 必須確認事項（ブロッカー）

- [ ] Japan East で ConversationTranscriber が動作することを確認
- [ ] Conversation Transcription の料金を確認し予算承認
- [ ] 一時停止/再開の代替実装（停止→再作成）の許容を確認
- [ ] 翻訳機能の2段階処理（認識→翻訳）への変更を許容

### 設計レビュー

- [ ] 話者IDがセッション間で維持されない制限をUIに明示
- [ ] 「話者識別中...」状態のUI設計完了
- [ ] エラーハンドリング仕様の確定
- [ ] 既存機能（TTS、保存、議事録）への影響評価完了

---

## 📋 概要

現在の「沈黙検出ベース」の話者識別を、**Azure Cognitive Services Conversation Transcription**を使用した**声紋ベースの話者識別**に置き換える。

### 現状の問題点

| 問題 | 詳細 |
|------|------|
| **誤検出** | 沈黙が2秒あるだけで話者が切り替わってしまう |
| **同一話者の分断** | 考え中の沈黙で別人として認識される |
| **連続発話の誤判定** | 会話のテンポが速いと同一話者として認識される |
| **精度不足** | 声の特徴を一切使っていない |

### 解決策: Azure Conversation Transcription

Azure Speech SDK の **Conversation Transcription API** を使用することで、以下が実現可能：

- ✅ **声紋（Voice Signature）** による話者識別
- ✅ 複数話者の**リアルタイム分離**
- ✅ 事前登録なしでの**動的話者クラスタリング**
- ✅ 最大**10人**までの話者識別

---

## 🔬 技術調査結果

### Azure Conversation Transcription の動作原理

```
┌─────────────────────────────────────────────────────────────┐
│                    Audio Input Stream                        │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│              Azure Speech Service                            │
│  ┌──────────────────┐  ┌──────────────────┐                 │
│  │ Speech-to-Text   │  │ Speaker          │                 │
│  │ Engine           │  │ Embedding Model  │                 │
│  └──────────────────┘  └──────────────────┘                 │
│           │                     │                            │
│           │    ┌────────────────┘                            │
│           ▼    ▼                                             │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ Speaker Diarization Engine                           │   │
│  │ - Voice feature extraction                           │   │
│  │ - Speaker clustering                                 │   │
│  │ - Turn segmentation                                  │   │
│  └──────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│ Output: { text, speakerId, timestamp, confidence }          │
└─────────────────────────────────────────────────────────────┘
```

### SDK クラス比較

| クラス | 用途 | 話者識別 |
|--------|------|---------|
| `SpeechRecognizer` | 単純な文字起こし | ❌ |
| `TranslationRecognizer` | 翻訳付き文字起こし | ❌ |
| `ConversationTranscriber` | 会話の文字起こし | ✅ 自動話者識別 |
| `Meeting` + `MeetingTranscriber` | 会議の文字起こし | ✅ 登録済み話者識別 |

### 推奨アプローチ

**ConversationTranscriber** を使用（事前登録不要、リアルタイム話者クラスタリング）

```typescript
// 概念コード
import * as SpeechSDK from "microsoft-cognitiveservices-speech-sdk";

const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(key, region);
const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();

// ConversationTranscriber を使用
const transcriber = new SpeechSDK.ConversationTranscriber(speechConfig, audioConfig);

transcriber.transcribed = (sender, event) => {
  const result = event.result;
  console.log(`Speaker: ${result.speakerId}`);  // "Guest-1", "Guest-2", etc.
  console.log(`Text: ${result.text}`);
  console.log(`Offset: ${result.offset}`);
};

transcriber.startTranscribingAsync();
```

---

## 📊 アーキテクチャ設計

### コンポーネント構成

```
┌─────────────────────────────────────────────────────────────┐
│                     React Component                          │
│                     (RecordingPage)                          │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│              useConversationTranscription Hook               │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ State:                                                  │ │
│  │ - isTranscribing: boolean                              │ │
│  │ - segments: TranscriptSegment[]                        │ │
│  │ - speakers: Map<string, SpeakerInfo>                   │ │
│  │ - currentSpeaker: string | null                        │ │
│  └────────────────────────────────────────────────────────┘ │
│  ┌────────────────────────────────────────────────────────┐ │
│  │ Methods:                                                │ │
│  │ - startTranscription()                                 │ │
│  │ - stopTranscription()                                  │ │
│  │ - renameSpeaker(id, name)                              │ │
│  └────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│          Azure Conversation Transcription API                │
│          (microsoft-cognitiveservices-speech-sdk)            │
└─────────────────────────────────────────────────────────────┘
```

### データモデル

```typescript
interface TranscriptSegment {
  id: string;
  speakerId: string;           // "Guest-1", "Guest-2", etc.
  speakerLabel: string;        // ユーザー設定のラベル（例: "田中さん"）
  text: string;
  timestamp: number;           // 開始時刻（ms）
  duration: number;            // 発話時間（ms）
  confidence: number;          // 認識信頼度 (0-1)
}

interface SpeakerInfo {
  id: string;                  // "Guest-1", etc.
  label: string;               // 表示名
  color: string;               // UI表示色
  segmentCount: number;        // 発話回数
  totalDuration: number;       // 総発話時間
}

interface ConversationTranscriptionState {
  isTranscribing: boolean;
  isPaused: boolean;
  segments: TranscriptSegment[];
  speakers: Map<string, SpeakerInfo>;
  currentSpeaker: string | null;
  interimText: string;
  error: string | null;
}
```

---

## 🎫 チケット詳細

### TICKET-060: ConversationTranscriber 基本実装
**優先度**: 🔴 最高 | **見積もり**: 4時間 | **依存**: なし

#### 概要
Azure Speech SDK の `ConversationTranscriber` を使用した基本的な話者識別付き文字起こし機能を実装する。

#### タスク
- [ ] `useConversationTranscription.ts` フック新規作成
- [ ] `ConversationTranscriber` の初期化処理
- [ ] `transcribing` イベント（中間結果）ハンドラ実装
- [ ] `transcribed` イベント（確定結果）ハンドラ実装
- [ ] `speakerId` の抽出と管理
- [ ] エラーハンドリング（`canceled` イベント）
- [ ] 一時停止/再開機能

#### 技術詳細

```typescript
// useConversationTranscription.ts
import * as SpeechSDK from "microsoft-cognitiveservices-speech-sdk";

export function useConversationTranscription(options: Options) {
  const transcriberRef = useRef<SpeechSDK.ConversationTranscriber | null>(null);
  
  const startTranscription = useCallback(() => {
    const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(
      options.subscriptionKey,
      options.region
    );
    speechConfig.speechRecognitionLanguage = options.language;
    
    // 話者識別を有効化（重要）
    speechConfig.setProperty(
      SpeechSDK.PropertyId.SpeechServiceConnection_LanguageIdMode,
      "Continuous"
    );
    
    const audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
    const transcriber = new SpeechSDK.ConversationTranscriber(
      speechConfig,
      audioConfig
    );
    
    // 中間結果
    transcriber.transcribing = (s, e) => {
      setInterimText(e.result.text);
      if (e.result.speakerId) {
        setCurrentSpeaker(e.result.speakerId);
      }
    };
    
    // 確定結果
    transcriber.transcribed = (s, e) => {
      if (e.result.reason === SpeechSDK.ResultReason.RecognizedSpeech) {
        const segment: TranscriptSegment = {
          id: generateId(),
          speakerId: e.result.speakerId || "Unknown",
          speakerLabel: getSpeakerLabel(e.result.speakerId),
          text: e.result.text,
          timestamp: e.result.offset / 10000, // 100ns → ms
          duration: e.result.duration / 10000,
          confidence: 1.0,
        };
        addSegment(segment);
        updateSpeakerStats(segment);
      }
    };
    
    transcriber.startTranscribingAsync();
    transcriberRef.current = transcriber;
  }, [options]);
  
  // ...
}
```

#### 完了条件
- [ ] 2人以上の話者が異なる `speakerId` で識別される
- [ ] リアルタイムで話者ごとにセグメントが分離される
- [ ] 中間結果と確定結果が正しく表示される
- [ ] エラー時に適切なメッセージが表示される

#### テスト方法
1. 2人で交互に話す
2. 同じ人が連続で話す（話者が変わらないことを確認）
3. 沈黙後に同じ人が話す（話者が変わらないことを確認）

---

### TICKET-061: 話者管理・ラベリング機能
**優先度**: 🔴 最高 | **見積もり**: 3時間 | **依存**: TICKET-060

#### 概要
識別された話者に対して、ユーザーがカスタムラベル（名前）を設定できる機能を実装する。

#### タスク
- [ ] `SpeakerInfo` 管理ロジック実装
- [ ] 話者ごとの色自動割り当て
- [ ] 話者名変更 UI（インラインエディット）
- [ ] 話者統計情報の計算（発話回数、総時間）
- [ ] LocalStorage への話者情報保存
- [ ] 話者一覧サイドパネル

#### UI設計

```
┌─────────────────────────────────────────────────────┐
│ 話者一覧                                    [折りたたむ] │
├─────────────────────────────────────────────────────┤
│ ● Guest-1  →  [田中さん     ] ✏️  │ 発話: 12回  │
│ ● Guest-2  →  [鈴木部長     ] ✏️  │ 発話: 8回   │
│ ● Guest-3  →  [新入社員A    ] ✏️  │ 発話: 3回   │
└─────────────────────────────────────────────────────┘
```

#### 完了条件
- [ ] 話者名をクリックで編集可能
- [ ] 変更した名前がすべてのセグメントに反映
- [ ] 次回起動時に話者名が復元される
- [ ] 話者ごとに異なる色で表示

---

### TICKET-062: 話者分離 UI コンポーネント
**優先度**: 🔴 最高 | **見積もり**: 4時間 | **依存**: TICKET-061

#### 概要
話者ごとに色分けされた会話ビューを実装する。

#### タスク
- [ ] `SpeakerSegment` コンポーネント作成
- [ ] 話者ごとの色テーマ定義（最大10色）
- [ ] タイムライン表示
- [ ] 話者フィルタ機能
- [ ] セグメントの結合表示（同一話者連続時）
- [ ] アニメーション（新規セグメント追加時）

#### UI設計

```
┌─────────────────────────────────────────────────────┐
│                    会話ログ                          │
├─────────────────────────────────────────────────────┤
│ 00:00:05  ┃  🟦 田中さん                            │
│           ┃  えーと、今日の議題についてですが、     │
│           ┃  まず予算の話からしましょう。           │
├───────────┼─────────────────────────────────────────┤
│ 00:00:15  ┃  🟩 鈴木部長                            │
│           ┃  はい、予算については前回の会議で       │
│           ┃  話した通り、20%削減の方向でお願いします。│
├───────────┼─────────────────────────────────────────┤
│ 00:00:28  ┃  🟦 田中さん                            │
│           ┃  承知しました。それでは具体的な         │
│           ┃  削減案を説明させていただきます。       │
└─────────────────────────────────────────────────────┘
```

#### 話者カラーパレット

```typescript
const SPEAKER_COLORS = [
  { bg: "bg-blue-100", border: "border-blue-400", text: "text-blue-800" },
  { bg: "bg-green-100", border: "border-green-400", text: "text-green-800" },
  { bg: "bg-purple-100", border: "border-purple-400", text: "text-purple-800" },
  { bg: "bg-orange-100", border: "border-orange-400", text: "text-orange-800" },
  { bg: "bg-pink-100", border: "border-pink-400", text: "text-pink-800" },
  { bg: "bg-cyan-100", border: "border-cyan-400", text: "text-cyan-800" },
  { bg: "bg-yellow-100", border: "border-yellow-400", text: "text-yellow-800" },
  { bg: "bg-red-100", border: "border-red-400", text: "text-red-800" },
  { bg: "bg-indigo-100", border: "border-indigo-400", text: "text-indigo-800" },
  { bg: "bg-teal-100", border: "border-teal-400", text: "text-teal-800" },
];
```

#### 完了条件
- [ ] 話者ごとに異なる背景色で表示
- [ ] タイムスタンプが表示される
- [ ] 同一話者の連続発話が視覚的にグループ化
- [ ] スムーズなスクロールアニメーション

---

### TICKET-063: 既存 useSpeechRecognition との統合
**優先度**: 🟡 高 | **見積もり**: 3時間 | **依存**: TICKET-062

#### 概要
既存の `useSpeechRecognition` フックを `useConversationTranscription` で置き換えるか、オプションで切り替え可能にする。

#### タスク
- [ ] `enableSpeakerDiarization` オプションの動作変更
  - `false`: 従来の `SpeechRecognizer` 使用
  - `true`: 新しい `ConversationTranscriber` 使用
- [ ] インターフェース互換性の維持
- [ ] 既存コンポーネントへの影響最小化
- [ ] フォールバック処理（API エラー時）

#### 移行戦略

```typescript
// useSpeechRecognition.ts
export function useSpeechRecognition(options: Options) {
  // 話者識別が有効な場合は ConversationTranscriber を使用
  if (options.enableSpeakerDiarization) {
    return useConversationTranscriptionInternal(options);
  }
  
  // 従来の SpeechRecognizer を使用
  return useSpeechRecognizerInternal(options);
}
```

#### 完了条件
- [ ] 既存機能が壊れない
- [ ] オプション切り替えで動作が変わる
- [ ] API エラー時に従来方式にフォールバック

---

### TICKET-064: 話者識別精度向上（Voice Profile 登録）
**優先度**: 🟢 中 | **見積もり**: 5時間 | **依存**: TICKET-063

#### 概要
事前に話者の Voice Profile を登録することで、識別精度を向上させる（オプション機能）。

#### タスク
- [ ] Voice Profile 登録 UI
- [ ] 登録用音声録音（20秒以上推奨）
- [ ] Azure Speaker Recognition API 連携
- [ ] Profile の保存・管理
- [ ] Meeting Transcriber への切り替え

#### 技術詳細

```typescript
// Voice Profile 登録
const profileClient = new SpeechSDK.VoiceProfileClient(speechConfig);
const profile = await profileClient.createProfileAsync(
  SpeechSDK.VoiceProfileType.TextIndependentIdentification,
  "ja-JP"
);

// 音声サンプルで学習
const audioConfig = SpeechSDK.AudioConfig.fromWavFileInput(audioFile);
const result = await profileClient.enrollProfileAsync(profile, audioConfig);

// Meeting Transcriber で使用
const meeting = SpeechSDK.Meeting.createMeetingAsync(speechConfig, meetingId);
const participant = SpeechSDK.Participant.from("user@example.com", "ja-JP", profile);
meeting.addParticipantAsync(participant);
```

#### 完了条件
- [ ] Voice Profile 登録フローが動作
- [ ] 登録済み話者が名前付きで識別される
- [ ] 未登録話者は "Guest-N" として識別される

---

### TICKET-065: パフォーマンス最適化
**優先度**: 🟢 中 | **見積もり**: 3時間 | **依存**: TICKET-063

#### 概要
長時間録音でもスムーズに動作するようパフォーマンスを最適化する。

#### タスク
- [ ] セグメントの仮想スクロール実装
- [ ] 大量セグメント時のメモリ管理
- [ ] バッチ更新による再レンダリング抑制
- [ ] Web Worker での音声処理（検討）

#### 完了条件
- [ ] 1000セグメント以上でもスムーズにスクロール
- [ ] メモリ使用量が線形増加しない
- [ ] 60fps を維持

---

## 📊 工数見積もり

| チケット | タイトル | 工数 | 優先度 |
|---------|---------|------|--------|
| TICKET-060 | ConversationTranscriber 基本実装 | 4h | 🔴 最高 |
| TICKET-061 | 話者管理・ラベリング機能 | 3h | 🔴 最高 |
| TICKET-062 | 話者分離 UI コンポーネント | 4h | 🔴 最高 |
| TICKET-063 | 既存フックとの統合 | 3h | 🟡 高 |
| TICKET-064 | Voice Profile 登録（オプション） | 5h | 🟢 中 |
| TICKET-065 | パフォーマンス最適化 | 3h | 🟢 中 |

### 合計工数

| 範囲 | 工数 |
|------|------|
| **MVP（TICKET-060〜063）** | **14時間** |
| フル実装（全チケット） | 22時間 |

---

## 📋 実装順序

```
TICKET-060 (ConversationTranscriber基本実装)
     │
     ▼
TICKET-061 (話者管理・ラベリング)
     │
     ▼
TICKET-062 (話者分離UI)
     │
     ▼
TICKET-063 (既存フック統合)
     │
     ├─→ TICKET-064 (Voice Profile) [オプション]
     │
     └─→ TICKET-065 (パフォーマンス最適化)
```

---

## 🔧 前提条件・制約

### Azure サービス要件

| 要件 | 詳細 |
|------|------|
| **SKU** | Speech Services S0（無料枠 F0 では話者識別制限あり） |
| **リージョン** | Conversation Transcription 対応リージョン |
| **言語** | 日本語（ja-JP）対応確認済み |

### 対応リージョン（Conversation Transcription）

- ✅ East US
- ✅ East US 2
- ✅ West US
- ✅ West US 2
- ✅ Central US
- ✅ Southeast Asia
- ✅ West Europe
- ⚠️ Japan East（確認必要）

### ブラウザ要件

| ブラウザ | 対応状況 |
|---------|---------|
| Chrome 80+ | ✅ 完全対応 |
| Edge 80+ | ✅ 完全対応 |
| Firefox 75+ | ✅ 対応 |
| Safari 14+ | ⚠️ 一部制限あり |

---

## 🧪 テスト計画

### 単体テスト

```typescript
describe("useConversationTranscription", () => {
  it("should identify different speakers", async () => {
    // 2人の異なる声を使ったテスト音声ファイルを使用
    const { result } = renderHook(() => useConversationTranscription(options));
    
    await act(async () => {
      result.current.startTranscription();
      // テスト音声を再生
      await playTestAudio("two-speakers.wav");
      result.current.stopTranscription();
    });
    
    const speakers = new Set(result.current.segments.map(s => s.speakerId));
    expect(speakers.size).toBeGreaterThanOrEqual(2);
  });
  
  it("should maintain same speaker for continuous speech", async () => {
    // 同一話者の連続発話テスト
  });
});
```

### E2E テスト

1. **2人会話シナリオ**: A→B→A→B の交互発話
2. **長時間独白シナリオ**: 1人が5分間話し続ける
3. **割り込みシナリオ**: 発話中に別の人が割り込む
4. **沈黙シナリオ**: 長い沈黙後に同じ人が話す

---

## 📚 参考資料

- [Azure Conversation Transcription ドキュメント](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-use-conversation-transcription)
- [Speaker Recognition API](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/speaker-recognition-overview)
- [Speech SDK JavaScript リファレンス](https://learn.microsoft.com/en-us/javascript/api/microsoft-cognitiveservices-speech-sdk/)
- [会話の文字起こしの概要](https://learn.microsoft.com/ja-jp/azure/ai-services/speech-service/conversation-transcription)

---

## 📝 変更履歴

| 日付 | バージョン | 変更内容 |
|------|-----------|---------|
| 2026-02-06 | 1.0 | 初版作成 |
