# 個人開発エンジニア向け面接回答集

> **プロジェクト**: Azure AI Voice Recorder & Real-time Translator
> **リポジトリ**: [hidetoshihonda/airecorder](https://github.com/hidetoshihonda/airecorder)
> **デプロイ先**: Azure Static Web Apps（本番稼働中）

---

## ① 超具体レベル（実装・コード・運用）

### Q1. その個人開発の概要を30秒で説明してください

> 「話すだけで、文字起こし・翻訳・議事録まで全部やってくれるWebアプリ」です。
>
> ブラウザを開いてボタンを押すだけで、Azure Speech Servicesによるリアルタイム音声認識、10言語対応の即時翻訳、話者識別、そしてGPT-4による議事録自動生成までワンストップで行えます。PLAUD NoteやDingTalk A1のようなAIボイスレコーダー端末の機能を、ハードウェア不要・ソフトウェアだけで実現するのがコンセプトです。Next.js + Azure Functions + Azure AI Servicesのフルサーバーレス構成で、月額$35〜50で運用しています。

---

### Q2. 使用している技術スタックは何ですか？なぜそれを選びましたか

| レイヤー | 技術 | バージョン |
|---------|------|-----------|
| フロントエンド | Next.js (App Router, Static Export) | 16.1.6 |
| UI | React 19 + TypeScript 5.x + Tailwind CSS 4.x + Radix UI | - |
| 音声SDK | microsoft-cognitiveservices-speech-sdk | 1.47.0 |
| バックエンド | Azure Functions v4 (Node.js 20) | - |
| データベース | Azure Cosmos DB (Serverless, NoSQL) | - |
| ストレージ | Azure Blob Storage | - |
| AI/翻訳 | Azure Speech Services / Azure Translator / Azure OpenAI (GPT-4) | - |
| ホスティング | Azure Static Web Apps (Standard) | - |
| 認証 | GitHub OAuth (SWA組み込み認証) | - |
| i18n | next-intl (日/英/西) | 4.8.2 |
| CI/CD | GitHub Actions (4ワークフロー) | - |

**選定理由:**

- **Next.js + Static Export**: SSRが不要なアプリなので、静的HTMLをCDN配信するだけで十分。Node.jsサーバー不要 → SWAの固定費$9/月のみで済む。SSR/ISRだとVercelやApp Serviceが必要でコストが跳ね上がる。
- **Azure統一**: Speech Services、Translator、OpenAIすべてAzureに揃えることで、認証・ネットワーク・課金を一元管理。マルチクラウドにすると運用複雑度が上がり、個人開発では不利。
- **Cosmos DB (Serverless)**: 録音データは「タイトル」「セグメント配列」「サマリーオブジェクト」とネストが深い。RDBだと複数テーブルに正規化が必要だが、NoSQLなら1ドキュメントにそのまま保存できる。Serverlessモードは使った分だけ課金で月$1〜5。
- **Radix UI**: ドロップダウンやダイアログなどのアクセシビリティ（WAI-ARIA）対応をライブラリに委譲し、見た目はTailwindで自由にカスタマイズ。Headless UIのメリット。
- **TypeScript**: フロントもバックも同一言語で統一。型安全によるバグ防止と、IDE補完による開発速度向上の両立。

---

### Q3. 一番苦労した実装箇所はどこですか？どう解決しましたか

**最も苦労したのは、Issue #35「Speech Translation SDK統合」での二重フック問題（BUG-3）です。**

**課題:**
翻訳機能にはSDKモード（TranslationRecognizer）とAPIモード（SpeechRecognizer + Translator REST API）の2つがあり、話者分離ON/OFFで切り替える設計にしました。しかし、Reactの Hooks ルールは「条件付きでフックを呼んではならない」ため、`if (mode === "sdk") useTranslationRecognizer()` のような書き方ができませんでした。

**解決策:**
両方のフックを**常に初期化**し、`translationMode`変数で「どちらのデータを使うか」を選択するStrategy パターンを採用しました。

```typescript
// 両方常に初期化（React Hooks ルール遵守）
const { transcript: sdkTranscript, ... } = useTranslationRecognizer(sdkConfig);
const { transcript: apiTranscript, ... } = useSpeechRecognition(apiConfig);

// モードに応じてデータソースを切り替え
const translationMode = enableSpeakerDiarization ? "api" : "sdk";
const transcript = translationMode === "sdk" ? sdkTranscript : apiTranscript;
```

使わない側のフックは`startListening`が呼ばれない限りリソース消費ゼロなので、パフォーマンスへの影響はありません。

**もう一つの大きな苦労: 録音状態のBoolean爆発（BUG-1〜7）**

複数の独立した`useState`（`isRecording`, `isPaused`, `isListening`）が組み合わさって矛盾状態が生じ、レースコンディションが頻発しました。これを**有限ステートマシン（FSM）**に全面リファクタリングし、9つの有効状態と明示的な遷移マップで解決しました。不正な遷移は静かに無視する設計で、ダブルクリック問題も同時に解消しています。

---

### Q4. 最近書いたコードで、改善したい箇所はどこですか

1. **`page.tsx`が約1,572行と巨大になっている点**。FSMによる状態管理、2つの翻訳モード、録音制御、UIレンダリングが1ファイルに集中しています。責務ごとに `RecordingControls`, `TranscriptView`, `TranslationPanel` などのコンポーネントに分割すべきです。

2. **テストがゼロである点**。CI/CDでビルド検証（lint + type check + build）は通していますが、ユニットテストは未実装です。特に `useSpeechRecognition` や `useTranslationRecognizer` のようなカスタムフックはReact Testing Libraryでテスト可能であり、SDKのモック化と合わせて導入したいです。

3. **Speech/Translator APIキーがフロントエンドの`NEXT_PUBLIC_*`環境変数に露出している点**。ブラウザからSDKで直接Azure接続する設計上やむを得ないですが、本番ではToken Serverを経由してSTS（Security Token Service）トークンを払い出す方式に切り替えるべきです。

---

### Q5. エラーハンドリングはどのように設計していますか

**3層構造のエラーハンドリング**を採用しています。

**① フック層（局所化）:**
各カスタムフック（`useSpeechRecognition`, `useTranslation`, `useAudioRecorder`, `useTranslationRecognizer`）が個別の`error`状態を持ち、SDK/APIのエラーをキャッチしてユーザー向けの日本語メッセージに変換します。
```typescript
const message = err instanceof Error ? err.message : "Unknown error";
setError(`認識エラー: ${message}`);
```

**② ページ層（集約・表示）:**
`page.tsx`で全フックのエラーをフィルタリングして統合表示します。
```typescript
const errors = [speechError, translationError, ttsError, audioError, fsmError].filter(Boolean);
```

**③ API層（統一レスポンス）:**
バックエンドの全APIエンドポイントは共通の`createJsonResponse`ヘルパーを経由し、CORSヘッダー付きの統一エラーレスポンスを返します。Cosmos DB固有の404エラーは`(error as { code?: number }).code === 404`で特定してnullを返す Graceful Degradation を実装しています。

**設計思想:** 翻訳APIが落ちても音声認識は継続する、OpenAIの議事録生成が失敗してもデータは保存される、という**部分的障害が全体に波及しない**設計を徹底しています。

---

### Q6. テストは書いていますか？どこまで・なぜその範囲ですか

**正直に言うと、自動テストは未実装です。** 現時点での品質担保はCI/CDの4ワークフローに依存しています。

| ワークフロー | 内容 |
|------------|------|
| PR Build Verification | ESLint + TypeScript型チェック + Next.jsビルド + APIビルド |
| Azure SWA CI/CD | Lint → Type check → Deploy（PRプレビュー環境含む） |
| Azure Functions CI/CD | Build → Deploy → **ヘルスチェック（5回リトライ）** |
| Azure Infra Health Check | 日次cron（JST 9:00）でSCM Basic Auth有効/EasyAuth無効を検証 |

**テストを書いていない理由（正直に）:**
- 初期は「まず動くものを出す」フェーズで、機能追加のスピードを優先した
- Azure Speech SDKのモック化が複雑で、テスト環境構築のROIが見合わないと判断した

**今後の計画:**
- カスタムフックのユニットテスト（React Testing Library + SDKモック）
- API関数の統合テスト（Jest + Cosmos DBエミュレータ）
- E2Eテスト（Playwright）— 録音→保存→履歴表示のクリティカルパス

これは反省点であり、チーム開発では初日からテストを書くべきだと考えています。

---

### Q7. パフォーマンスで意識した点はありますか

**4つの主要な最適化を行っています。**

**① セグメント単位差分翻訳（Issue #33）:**
元々は文字起こし全文をまるごと翻訳APIに送っていましたが、30分の会議だと毎回数万文字を翻訳し直すことになりコストも遅延も膨大でした。確定済みセグメントのIDを`Set`でキャッシュし、新規セグメントだけを翻訳する差分方式にリファクタリングしました。API呼び出し回数が約1/10に削減されました。

**② SDK統合によるゼロレイテンシ翻訳（Issue #35）:**
TranslationRecognizerを使うことで音声認識と翻訳を単一パイプラインに統合し、REST API往復の300〜800msを完全に排除しました。話者分離が不要な場面ではSDKモードが自動選択されます。

**③ 中間結果のデバウンス:**
音声認識の`recognizing`イベントは数十ms間隔で発火するため、翻訳API呼び出しを300msでデバウンスしてAPI flooding を防止しています。

**④ requestAnimationFrameによるスムーズスクロール:**
翻訳結果表示エリアの自動スクロールに`requestAnimationFrame`を使用し、DOM更新とスクロールのタイミングを同期させています。

---

### Q8. セキュリティで最低限意識していることは何ですか

**多層防御（Defense in Depth）を5段階で実装しています。**

| 層 | 実装 |
|----|------|
| ① SWAルート設定 | `staticwebapp.config.json`で`/api/*`を`authenticated`ロールに制限。未認証は302リダイレクト |
| ② API認証ミドルウェア | `x-ms-client-principal`ヘッダーをBase64デコードしてuserIdを取得。ヘッダー欠損時は401 |
| ③ 所有者チェック | 録音データの`userId`とリクエスト者の`userId`を照合。不一致は403 Forbidden |
| ④ Cosmos DBパーティション | `userId`がパーティションキーなので、物理的にユーザー間データが分離 |
| ⑤ Blob SAS Token | アップロード用は30分有効・書込のみ、ダウンロード用は1時間有効・読取のみの最小権限 |

加えて、`staticwebapp.config.json`のグローバルヘッダーで `X-Content-Type-Options: nosniff`, `X-Frame-Options: DENY`, `X-XSS-Protection: 1; mode=block`, `Permissions-Policy: microphone=(self)` を設定し、ブラウザレベルの基本的な攻撃を防いでいます。

Cosmos DBクエリはパラメータ化（`@search`, `@userId`）でSQLインジェクションを防止しています。

---

### Q9. 本番障害が起きたことはありますか？どう対応しましたか

**はい。Azure Static Web Appsの認証設定ミスによる全面停止を経験しました。**

**インシデント:** Azure Portalからの操作で意図せずEasyAuth（App Service Authentication）が有効化され、SWA組み込み認証と競合。すべてのページで認証ループが発生し、ユーザーがアクセス不能になりました。

**対応:**
1. Azure CLIで`az staticwebapp appsettings`を確認 → EasyAuth設定を検出
2. 設定を無効化して復旧
3. **再発防止策として日次ヘルスチェックワークフロー（`infra-health-check.yml`）を追加**。毎朝JST 9:00にcronで「ヘルスエンドポイント応答確認」「SCM Basic Auth有効確認」「EasyAuth無効確認」を自動実行し、異常時はGitHub Actionsの失敗通知が飛ぶようにしました。

**学び:** クラウドサービスは「設定1つ」で全面障害が起きる。手動確認に頼らず、**Infrastructure as Code + 自動監視**で構成ドリフトを検出する仕組みが不可欠だと痛感しました。

---

### Q10. リファクタリングはどのタイミングで行いますか

**「3回目の修正で抽象化」を基本ルールにしています。**

**実例①: FSMリファクタリング**
録音制御で`isRecording`, `isPaused`, `isListening`の3つのBooleanを個別管理していたところ、Issue #2, #4, #9と3回にわたって状態矛盾のバグが報告されました。3回目で「これはBoolean管理の限界だ」と判断し、有限ステートマシンに全面リファクタリング。9状態×明示的遷移マップに統一しました。

**実例②: 差分翻訳リファクタリング（Issue #33）**
最初の翻訳実装は全文再翻訳方式。Issue #33の分析で「文字数ベースのスライスが文の途中で切れる」バグを発見し、セグメント単位のキャッシュ付き差分翻訳に再設計しました。

**タイミングの判断基準:**
- 同じ箇所で3回バグが出たら → 設計の問題として抽象化
- 新機能追加時に既存コードが邪魔になったら → 先にリファクタ
- PRレビュー（AIエージェント）で指摘されたら → 即対応

---

## ② 設計・構造レベル（設計思想・判断理由）

### Q11. 全体のアーキテクチャを図で説明できますか

```
┌───────────────────────────────────────────────────────────┐
│              ユーザーのブラウザ                             │
│  ┌─────────────────────────────────────────────────────┐  │
│  │  Next.js (React) フロントエンド [Static Export]      │  │
│  │  ├─ MediaRecorder API (音声キャプチャ)               │  │
│  │  ├─ Speech SDK (WebSocket直結)                       │  │
│  │  ├─ TranslationRecognizer (SDK翻訳) ←── Issue #35   │  │
│  │  └─ Translator REST API (API翻訳)   ←── Issue #33   │  │
│  └─────────────────────────────────────────────────────┘  │
└───────────────────────────────────────────────────────────┘
         │WebSocket        │REST          │REST (認証Cookie)
         ▼                 ▼              ▼
┌──────────────┐ ┌──────────────┐ ┌─────────────────────────┐
│Azure Speech  │ │Azure         │ │Azure Static Web Apps    │
│Services      │ │Translator    │ │ ├─ GitHub OAuth認証      │
│├─ STT        │ │              │ │ ├─ CDN配信              │
│├─ 話者識別   │ │              │ │ └─ BYOF → Azure Functions│
│└─ Translation│ │              │ └─────────────────────────┘
└──────────────┘ └──────────────┘           │
                                            ▼
                                 ┌──────────────────────┐
                                 │Azure Functions (API)  │
                                 │├─ recordings CRUD     │
                                 │├─ summary (GPT-4)     │
                                 │└─ blob SAS URL発行    │
                                 └──────────────────────┘
                                     │        │        │
                          ┌──────────┘        │        └──────────┐
                          ▼                   ▼                   ▼
                ┌──────────────┐  ┌──────────────────┐  ┌──────────────┐
                │Cosmos DB     │  │Blob Storage      │  │Azure OpenAI  │
                │(メタデータ)  │  │(音声ファイル)    │  │(GPT-4)       │
                │userId分離    │  │SAS Token直結     │  │議事録生成    │
                └──────────────┘  └──────────────────┘  └──────────────┘
```

**特徴: 「クライアントヘビー」な BaaS寄り構成**
- 音声認識・翻訳はブラウザから直接Azure AIサービスへWebSocket/REST接続（レイテンシ最小化）
- APIキーの秘匿が必要な処理（GPT-4呼び出し、DB操作、SAS発行）だけAzure Functions経由
- 自分で管理するサーバーは実質Azure Functionsのみ。残りは全てマネージドサービスに委譲

---

### Q12. 責務分離はどのように考えていますか

**フロントエンドはカスタムフックパターンでUI/ロジックを完全分離しています。**

| フック | 責務 |
|--------|------|
| `useAudioRecorder` | マイク制御・音声キャプチャ・MediaStream管理 |
| `useSpeechRecognition` | Azure Speech SDK接続・文字起こし・話者識別 |
| `useTranslation` | Translator REST API呼び出し・差分翻訳キャッシュ |
| `useTranslationRecognizer` | TranslationRecognizer SDK接続（認識+翻訳統合） |
| `useRecordingStateMachine` | 録音状態FSM管理 |

`page.tsx`はこれらフックのオーケストレーション（モード切替・データ統合・UI描画）に徹しています。

**バックエンドはController-Service-Repositoryパターン:**
- `functions/*.ts` → HTTPリクエスト受付・バリデーション・レスポンス整形（Controller）
- `services/*.ts` → ビジネスロジック・Cosmos DB/Blob操作（Service + Repository）
- `models/*.ts` → データ型定義

---

### Q13. なぜそのデータベース設計にしましたか

**Cosmos DB (NoSQL, Serverless) を選んだ3つの理由:**

**① スキーマの柔軟性:** 録音データは`segments[]`（話者別セグメント配列）、`summary`（構造化議事録オブジェクト）、`translatedSegments[]`（翻訳セグメント配列）など、深くネストされた構造です。RDBだと`recordings`, `segments`, `summaries`, `action_items`と4テーブル以上に正規化が必要ですが、NoSQLなら1ドキュメントにそのまま格納できます。

**② パーティションキー設計:** `userId`をパーティションキーに設定。このアプリのクエリは100%「自分の録音を取得」なので、`userId`で絞り込むと1パーティションだけ読めばよく、ファンアウトなしで高速です。物理的なデータ分離によるマルチテナント分離も同時に実現。

**③ コスト:** Serverlessモードは使った分だけ課金。個人開発レベルのアクセス量では月$1〜5。プロビジョニングモード（最低400 RU/s固定で月$24〜）と比較して圧倒的に安い。

---

### Q14. フレームワークの規約と独自実装の線引きは？

**原則: フレームワークが提供する機能は最大限活用し、独自実装は「フレームワークではカバーできない領域」に限定する。**

| 領域 | 採用したフレームワーク機能 |
|------|--------------------------|
| ルーティング | Next.js App Router（ファイルベース） |
| 認証 | SWA組み込みOAuth（コードゼロ） |
| 国際化 | next-intl（メッセージファイルベース） |
| スタイリング | Tailwind CSS（ユーティリティクラス） |
| UIコンポーネント | Radix UI（Headless、アクセシビリティ自動） |

**独自実装した領域:**
- カスタムフック群（Azure Speech SDKのReactラッパーは存在しないため）
- 有限ステートマシン（`useRecordingStateMachine`）— 汎用ライブラリ(XState等)は重すぎると判断
- セグメント差分翻訳キャッシュ — ビジネスロジック固有

**判断基準:** 「そのライブラリを導入することで、バンドルサイズやメンテナンスコストがメリットを上回らないか」を検討。例えばXStateは録音状態管理だけには過剰なので、9状態の遷移マップを自前で120行で実装しました。

---

### Q15. スケールするとしたら、どこがボトルネックになりますか

**3つのボトルネックを特定しています。**

**① Azure Functions Consumption Plan のコールドスタート:**
使われていない時間帯のあとの初回リクエストで数秒の遅延が発生します。ユーザー数が増えてレスポンスタイムのSLAが必要になったら、Premium PlanまたはAlways-On設定への切り替えが必要です。

**② Speech Services のリクエスト集中:**
S0 SKU には秒間同時接続数の上限があります。100人が同時に音声認識を開始した場合、429（Rate Limit）が返る可能性があります。対策はAzure Speech Services のスケールアップまたはリージョン分散。

**③ `page.tsx`のクライアントサイド状態管理:**
現在は1つのコンポーネントに全状態が集中しており、1,572行に膨らんでいます。多機能化が進めば、Zustand等のグローバルステート管理や、コンポーネント分割によるレンダリング最適化が必要です。

---

### Q16. 技術選定時に他の選択肢は検討しましたか

| 選択 | 検討した代替案 | 不採用理由 |
|------|---------------|-----------|
| Azure Speech Services | Google Cloud STT, AWS Transcribe | Translator・OpenAIとAzure統一でコスト・管理をシンプルに |
| Cosmos DB | PostgreSQL (Supabase) | ネストの深いJSONスキーマにはNoSQLが自然。RDBだとJOINが増え複雑化 |
| SWA組み込み認証 | Auth0, Clerk, Firebase Auth | コード不要・無料・SWAとネイティブ統合。Auth0は月$23〜で個人には高い |
| Static Export | Vercel (SSR) | Vercelは無料枠がある点は魅力だが、Azure統一を優先。SSR不要なのでSWA+CDNで十分 |
| Azure Functions | Express.js on App Service | サーバーレスの従量課金メリット。API 5本程度なら固定費不要のFunctionsが最適 |
| 自前FSM | XState | 9状態程度ならXStateの学習コスト・バンドルサイズが見合わない。120行で自作 |

---

### Q17. 将来変更を見据えて工夫している点は？

**① 翻訳モードのStrategy化:** SDK/APIの2モードを`translationMode`フラグで切り替える設計にしたことで、将来3つ目の翻訳エンジン（例: Google Cloud Translation）を追加しても`page.tsx`の変更は最小限で済みます。

**② データモデルの後方互換:** `Summary`型は古い形式（`overview`, `keyPoints`のフラット構造）と新しい形式（`meetingInfo`, `topics[]`, `actionItems[]`のリッチ構造）の両方をサポートしています。既存データを壊さずに段階的にマイグレーション可能です。

**③ テンプレートシステム:** 議事録テンプレートは`preset`（プリセット）と`custom`（ユーザー作成）を`type`フィールドで区別し、Cosmos DBに保存。テンプレート追加がコード変更なしで可能です。

**④ i18n設計:** `next-intl`でメッセージファイル（`ja.json`, `en.json`, `es.json`）を分離。新言語追加はJSONファイル1つ追加するだけです。

---

### Q18. 自分なりの良い設計・悪い設計の基準は？

**良い設計の基準:**
- **変更が局所的**に済む — 翻訳ロジックを変えてもUIに影響しない（カスタムフック分離）
- **不正な状態が表現できない** — FSMにより「録音中かつ停止中」のような矛盾状態が型レベルで防がれる
- **読む人が意図を理解できる** — `translationMode === "sdk"` と書けば、コメントなしで何をしているか分かる

**悪い設計の基準:**
- **Booleanの組み合わせで状態を管理する** — 2つで4状態、3つで8状態。大半が無効な状態で、バグの温床になる（実際にFSM前に経験した）
- **1ファイルに複数の責務が混在** — 現在の`page.tsx`がまさにこの状態。認識はしているが段階的に改善中
- **暗黙の依存関係** — 「この関数はあの関数の後に呼ばないと動かない」のような順序依存。明示的なインターフェースで解消すべき

---

## ③ 問題解決・思考プロセスレベル

### Q19. この個人開発は、どんな課題意識から始めましたか

**PLAUD NoteやDingTalk A1のようなAIボイスレコーダー端末は1万〜3万円するハードウェアが必要です。**しかし、これらの端末が提供する核心機能——音声認識、翻訳、AI要約——はすべてクラウドAI APIで実現可能です。

「ハードウェアを買わなくても、ブラウザだけで同じ体験ができるのではないか？」という課題意識から開発を始めました。

加えて、個人的に**Azureのフルスタックアーキテクチャ**（Speech Services, OpenAI, Cosmos DB, Functions, SWAを組み合わせた実践的な構成）を深く学びたいという技術的モチベーションもありました。

---

### Q20. 課題はどのように言語化・分解しましたか

**10製品の競合分析から始めて、機能要件を体系化しました。**

1. PLAUD Note, DingTalk A1, Notta Memo, AutoMemo S等の10製品を詳細調査（[機能分析ドキュメント](docs/機能分析.md)として文書化）
2. ハードウェア固有の機能（骨伝導マイク等）と**ソフトウェアで再現可能な機能**を分離
3. 再現可能な機能を優先度（必須/推奨/任意）で分類した[要件定義書](docs/要件定義書.md)を作成
4. 要件をGitHub Issuesに分解（110+件）。各Issueに「分析レビュー → 実装計画書 → 実装 → PR → デプロイ」のワークフローを確立

---

### Q21. 実装前にどんな仮説を立てましたか

**主な仮説と検証結果:**

| 仮説 | 検証結果 |
|------|---------|
| 音声認識はブラウザから直接WebSocket接続すべき（API経由では遅延増大） | ✅ 正解。Speech SDKがWebSocket管理を自動化し、50ms以下のレイテンシを実現 |
| Static Exportで十分（SSR不要） | ✅ 正解。初期表示にサーバーサイドデータフェッチがなく、CDN配信で高速 |
| 翻訳はREST APIで1文ずつ呼べば十分 | ❌ 不十分。セグメント差分翻訳（Issue #33）とSDK統合（Issue #35）が必要だった |
| Boolean管理で録音状態を制御できる | ❌ 破綻。3つのBooleanの組み合わせで矛盾状態が頻発し、FSMにリファクタリング |
| Cosmos DB Serverlessは個人開発に最適 | ✅ 正解。月$1〜5で十分な性能。RDBの固定費と比較して圧倒的に安い |

---

### Q22. 途中で方向転換した経験はありますか？なぜですか

**最大の方向転換は翻訳アーキテクチャです。**

**初期設計（v1）:** 音声認識の確定テキスト全文を、毎回Translator REST APIに送信
→ **問題:** 会議が長くなるとAPI呼び出しごとに数万文字を翻訳。コスト増大+レイテンシ悪化

**リファクタリング①（Issue #33）:** セグメント単位の差分翻訳。翻訳済みセグメントIDをSetでキャッシュし、新規のみ翻訳
→ **改善:** API呼び出し1/10に。ただしREST往復300〜800msの遅延は残る

**リファクタリング②（Issue #35）:** TranslationRecognizer SDK統合。認識+翻訳を単一パイプラインに
→ **改善:** REST往復を完全排除。ゼロレイテンシ翻訳を実現。ただし話者分離と排他なので、APIモードとのデュアル構成に

**方向転換の判断基準:** 「ユーザー体験に直結するレイテンシの問題」が明確になった時点で、アーキテクチャレベルの変更を決断しました。

---

### Q23. 想定と違った点は何でしたか

1. **Azure Speech SDKにpause/resume APIが存在しない**こと。`SpeechRecognizer`も`ConversationTranscriber`もpauseメソッドがなく、stop→再create→startで擬似的にpauseを実装する必要がありました。`isPausedRef`フラグで意図的なpauseとセッション終了を区別しています。

2. **Speech ServicesとTranslator APIで言語コード体系が異なる**こと。同じMicrosoft製品なのに、Speechは`ja-JP`（BCP 47）、Translatorは`ja`（ISO 639-1）。さらにTranslationRecognizer SDKは`ja`ではなく独自の`translatorCode`（例: 中国語は`zh-Hans`）が必要。アダプター層を作って吸収しました。

3. **SWAの認証設定が脆弱**なこと。Azure Portal上の操作ひとつでEasyAuthが誤って有効化され、全面停止インシデントが発生。「設定は必ずコードで管理し、自動監視で検証する」という教訓を得ました。

---

### Q24. 技術的に詰まったとき、どうやって調べますか

**3段階のアプローチを取ります。**

1. **公式ドキュメント優先:** Azure Speech SDK, Next.js, Cosmos DBの公式ドキュメントを最初に確認。特にAzureは公式サンプルコードが充実しており、8割はここで解決します。

2. **ソースコードを読む:** Speech SDKはnpmパッケージの型定義（`.d.ts`）からメソッドシグネチャと JSDocを確認。Next.jsの挙動は[GitHub リポジトリ](https://github.com/vercel/next.js)のissueを検索。

3. **AIアシスタント活用:** GitHub Copilotで実装パターンの叩き台を生成し、自分でレビュー・修正。AIエージェント（Serena）を分析・実装・デプロイの各フェーズで活用し、Issue分析レビューや実装計画書の作成を効率化しています。

**詰まった具体例:** TranslationRecognizerの`addTargetLanguage`に渡す言語コードが不明だった際、SDK型定義から追い、Azure公式ドキュメントの「Supported languages」ページで`translatorCode`の存在を発見しました。

---

### Q25. 他人のコードやOSSから、何を学びましたか

- **Next.js App Router のファイルベースルーティング設計:** `app/recording/page.tsx` → `/recording` のような直感的なURL設計。Layout compositionパターンで共通UIをネスト。

- **Radix UI のHeadless UIパターン:** 「ロジック（キーボード操作、ARIA、フォーカス管理）はライブラリが担当、見た目は開発者が担当」という責務分離の美しさ。自分のカスタムフックもこのHeadless思想を参考にしています。

- **Azure Speech SDK のイベントドリブン設計:** `recognizing`（中間）と`recognized`（確定）の2段階イベントモデル。リアルタイムフィードバック（灰色テキスト）と確定データ（保存対象）を明確に分離する設計パターンを学びました。

---

## ④ プロダクト・ユーザー視点（価値創出）

### Q26. 誰の、どんな課題を解決するプロダクトですか

**ビジネスパーソンの「会議のあとの面倒な作業」を自動化するプロダクトです。**

- 会議中にメモを取りながら聞くのは困難 → リアルタイム文字起こしで解決
- 多言語の会議で翻訳が追いつかない → 10言語リアルタイム翻訳で解決
- 議事録作成に30分〜1時間かかる → GPT-4が1クリックで生成
- 専用ハードウェアが1〜3万円する → ブラウザだけで無料（ランニングコストのみ）

---

### Q27. ユーザー像（ペルソナ）はありますか

**主要ペルソナ3つ:**

| ペルソナ | 属性 | 主な利用シーン |
|---------|------|--------------|
| ① グローバル企業のビジネスパーソン | 30代、日英の会議が多い | 海外チームとのミーティング議事録 |
| ② インタビュアー・記者 | フリーランス、取材が多い | インタビュー録音→文字起こし→記事下書き |
| ③ 語学学習者 | 20代、英語学習中 | 自分の発話を文字起こし+翻訳で確認 |

要件定義書にも明記しています。ターゲットはGitHubアカウントを持つ層（エンジニア・テック寄りのビジネスパーソン）に限定し、GitHub OAuth認証を採用しました。

---

### Q28. 実際に使われていますか？数字はありますか

現時点では個人利用+クローズドなテスト段階です。本番環境はAzure Static Web Appsにデプロイ済みで稼働中ですが、パブリックな集客はまだ行っていません。

**開発の実績数字:**
- GitHub Issues: 110+件
- マージ済みPR: 118件
- CI/CDワークフロー: 4本（全自動デプロイ）
- 対応言語: 10言語（音声認識）/ 3言語（UI）
- ドキュメント: 47ファイル（分析レビュー+実装計画書）

今後、社内ツールとしての利用やProduct Huntでの公開を検討しています。

---

### Q29. フィードバックはどのように集めましたか

**GitHub Issues ベースのフィードバックループを構築しています。**

- 自身の使用中に発見した問題をIssueに起票（例: Issue #103「文字起こし補正失敗」、Issue #104「再生速度選択状態が保持されない」）
- 各Issueに対して「分析レビュー → 実装計画書 → 実装 → PR → デプロイ」の標準ワークフローを適用
- AIエージェント（GitHub Copilot + Serena）による分析レビュー文書の自動生成で、問題の根本原因分析を効率化

ユーザーテストの計画として、社内の多言語チームに使ってもらい、Slackでの定性フィードバック + Azure Application Insightsでの定量データ収集を予定しています。

---

### Q30. ユーザーの声を受けて改善した例は？

**自身の使用体験から改善した具体例:**

- **Issue #104: 再生速度が選択状態を保持しない** → 変速再生UIの状態管理を修正（Issue #78で実装した変速再生機能の品質改善）
- **Issue #105: リアルタイム翻訳が長文化すると遅延する** → セグメント差分翻訳（Issue #33）で根本解決
- **Issue #106: カスタムテンプレートが作成できない** → Cosmos DB操作のバリデーション修正
- **Issue #107/108: 設定画面のバグ** → UIコンポーネントの状態リセット処理を修正

すべてのIssueに分析レビュー文書を作成し、根本原因の特定→修正→検証のサイクルを回しています。

---

### Q31. 「技術的に面白い」と「ユーザー価値」のバランスをどう考えていますか

**「ユーザー価値が先、技術的面白さは手段」を原則にしています。ただし、個人開発では意図的に技術チャレンジを選ぶことも許容しています。**

**例: Issue #35 (TranslationRecognizer SDK統合)**
- ユーザー価値: 翻訳の体感遅延がゼロになる（300ms→0ms）
- 技術的面白さ: Speech SDKの高度なAPIを使いこなす、デュアルモードアーキテクチャの設計

この場合はユーザー価値と技術的面白さが一致しましたが、もし「技術的には面白いが遅延が増える」選択肢だったら採用しません。

**逆に、FSMリファクタリングは外見上ユーザーには見えない改善ですが**、状態矛盾バグの根絶というユーザー価値（安定動作）を間接的に実現しています。

---

### Q32. もし失敗しているなら、なぜだと思いますか

**現時点での最大の課題は「ユーザー獲得の検証が不十分」なことです。**

技術的にはフル機能で動作していますが、「本当にブラウザだけでAIボイスレコーダーを使いたい人がどれだけいるか」の市場検証をしていません。PLAUD Noteが$169で売れている事実は、「物理デバイスの手軽さ・常時携帯性」に価値があるからかもしれず、ソフトウェアだけでは代替できない部分がある可能性があります。

次のステップとして、限定的なユーザーテストで「Web版で十分か、物理デバイスが欲しいか」の定性フィードバックを収集すべきだと考えています。

---

## ⑤ 抽象レベル（学び・成長・再現性）

### Q33. この個人開発で得た最大の学びは何ですか

**「状態管理の設計を間違えると、すべてが崩壊する」ということです。**

Boolean 3つの組み合わせで録音状態を管理していたとき、あらゆる新機能追加でレースコンディションが発生し、修正のたびに別の箇所が壊れる状態になりました。FSMに全面リファクタリングして、「正しい状態遷移しか起こり得ない」設計にしたことで、以降のバグ報告が劇的に減りました。

**技術的学び:** 「不正な状態を型レベルで表現不能にする」（Make Illegal States Unrepresentable）は理論として知っていましたが、自分のコードで痛みを伴って体験したことで、血肉になりました。

**もう一つの学び:** Azureのサーバーレス構成（Functions + Cosmos DB Serverless + SWA）を実際に構築・運用することで、「個人開発でもエンタープライズ級のインフラが月$35〜50で手に入る」という感覚が身につきました。

---

### Q34. 仕事にどう活かせると思いますか

**3つの領域で直接活かせます。**

**① クラウドアーキテクチャ設計:** Azure Functions, Cosmos DB, Speech Services, OpenAIの実運用経験。特にCosmos DBのパーティション設計やSASトークンパターンは、多くのAzure案件で使われる基本パターンです。

**② リアルタイムアプリケーション開発:** WebSocket接続管理、ストリーミングデータ処理、中間結果/確定結果の分離、デバウンス・スロットリングなど、リアルタイムシステムの実装パターンを一通り経験しました。

**③ AI統合の実務知識:** GPT-4のプロンプトエンジニアリング（JSON出力強制、テンプレート切替）、Speech SDKのイベントモデル、APIキー管理（フロントvs.バック）など、AI API統合の実践ノウハウ。

---

### Q35. 今やり直すなら、何を変えますか

1. **最初からFSMで状態管理する。** Booleanの組み合わせで始めた結果、途中で全面リファクタリングが必要になった。最初から「状態は有限で、遷移は明示的」の原則で設計すれば、リファクタリングコストを回避できた。

2. **最初からテストを書く。** CIの型チェック+ビルド検証だけでは、ロジックの正しさを保証できない。特にカスタムフックのユニットテストは初日から書くべきだった。

3. **Token Serverを初期から実装する。** Speech/TranslatorのAPIキーをフロントエンドに露出する設計は、プロトタイプとしては速いが本番には不適切。最初からSTSトークン方式にしておけば、後からの移行コストがゼロだった。

4. **`page.tsx`を初期からコンポーネント分割する。** 1ファイルが1,572行になってから分割するのは大変。最初から`RecordingControls`, `TranscriptView`, `TranslationPanel`と分けておけばよかった。

---

### Q36. 他人が同じものを作るなら、どんなアドバイスをしますか

1. **まずARCHITECTURE.mdを書く。** 全体像を可視化してからコードを書く。本プロジェクトの794行のアーキテクチャ文書は、実装中の迷いを大幅に減らしました。

2. **Azure Functionsは最小限に。** フロントエンドから直接呼べるAPIは直接呼ぶ。Functions経由が必要なのは「APIキーを秘匿したい処理」だけ。過度にFunctions を経由するとレイテンシもコストも増える。

3. **Speech SDKのpause未対応を前提に設計する。** stop→re-create→startパターンは設計上の前提として最初から組み込むこと。後から知ると手戻りが大きい。

4. **状態管理はBooleanではなくEnumかFSMで。** 2つ以上のBooleanを組み合わせる設計は必ず破綻する。

5. **Cosmos DBのパーティションキーは最初に決める。** 後から変更はほぼ不可能（コンテナ再作成が必要）。

---

### Q37. 自分のエンジニアとしての強み・弱みは何だと思いますか

**強み:**
- **設計文書化の徹底:** 110+のIssueそれぞれに分析レビューと実装計画書を作成。思考過程を文書として残す習慣が、品質と再現性を担保している
- **Azureフルスタックの実践知識:** Speech, OpenAI, Cosmos DB, Functions, SWAを統合した実運用経験
- **問題分解力:** 漠然とした「翻訳が遅い」という問題を「全文再翻訳→差分翻訳→SDK統合」と段階的に分解・解決できる

**弱み:**
- **テスト文化の不足:** 自動テストを後回しにする癖がある。チーム開発ではTDD/テストファーストを意識的に実践する必要がある
- **UI/UXデザイン力:** 機能は作れるが、ユーザー調査に基づいたUIデザインの経験は浅い。デザイナーとの協業スキルを伸ばしたい
- **コンポーネント粒度の判断:** 1ファイルにロジックを集中させがちで、分割のタイミングが遅い

---

### Q38. 個人開発とチーム開発で、何が一番違うと思いますか

**「合意形成のコスト」が桁違いに異なります。**

個人開発ではFSMに全面リファクタリングする判断を5分で下せますが、チームでは「なぜBooleanではダメなのか」「FSMの学習コストは許容範囲か」「既存テストへの影響は」を関係者と合意する必要があります。

**個人開発で得た設計力をチーム開発に活かすには:**
- 「なぜこの設計にしたか」をドキュメントで説明できる力（本プロジェクトの47ファイルの分析レビュー・実装計画書はその訓練）
- PRの粒度を小さく保ち、レビュー負荷を下げる工夫
- 命名規則やコーディング規約を最初に合意する重要性

**逆に、個人開発の危険性:** コードレビューがないため、独りよがりな設計に気づけない。本プロジェクトではAIエージェントをレビュワーとして活用することで、この問題を軽減しています。

---

## ⑥ エンジニア観・価値観レベル（最抽象）

### Q39. あなたにとって良いエンジニアとは何ですか

**「なぜそうしたか」を説明できるエンジニアです。**

コードを書ける人は多いですが、「なぜCosmos DBでありPostgreSQLではないのか」「なぜBooleanではなくFSMなのか」「なぜAPIキーをバックエンドに置くのか」を技術的根拠とトレードオフを含めて説明できる人は少ないと思います。

本プロジェクトのARCHITECTURE.md（794行）を書いたのは、自分自身がその「なぜ」を言語化し、将来の自分や他者に説明できるようにするためです。

---

### Q40. なぜ個人開発を続けているのですか

**3つの理由があります。**

1. **技術の検証場として:** 仕事では新技術の採用にリスク判断が必要ですが、個人開発なら「Next.js 16 + React 19 + Tailwind CSS v4」のような最新スタックを制約なく試せます。実際に動くアプリで検証したスキルは、仕事で提案する際の説得力になります。

2. **設計判断の全責任を負う経験として:** チームでは部分的な設計しか担当しないことが多いですが、個人開発ではインフラからフロント・バック・CI/CDまで全レイヤーの設計判断を自分で下し、その結果（成功も失敗も）を引き受けます。

3. **アウトプットの習慣として:** 118件のPR、47件の設計文書は「考える→作る→振り返る」サイクルの蓄積です。この習慣はチーム開発でも自然に発揮されます。

---

### Q41. 技術選択で流行と本質をどう見分けますか

**「5年後にも使われているか」を基準にします。**

| 本質（選んだもの） | 理由 |
|-------------------|------|
| React | 10年の実績。コンポーネントモデルはUI開発の本質的パラダイム |
| TypeScript | 型安全は一時的な流行ではなく、ソフトウェア品質の基礎 |
| REST API | シンプル・普遍的。GraphQLは要件に合えば使うが、このアプリでは過剰 |
| サーバーレス | 「使った分だけ払う」は個人開発の本質的ニーズに合致 |

| 流行に振り回されなかったもの | 理由 |
|-----------------------------|------|
| Server Components | このアプリはStatic Exportで十分。RSCの恩恵を受ける場面がない |
| tRPC | APIが5本だけの小規模アプリでは型安全RPCの学習コストが見合わない |
| Bun | Node.js 20で十分安定。Azure FunctionsのBun対応も未成熟 |

**判断プロセス:** 「その技術がなければ解決できない問題があるか？」を問う。なければ、より枯れた技術を選ぶ。

---

### Q42. お金・技術的挑戦・ユーザー価値の優先順位は？

**ユーザー価値 > 技術的挑戦 > お金。ただし、個人開発のフェーズによって比重は変わります。**

- **現在（プロトタイプ〜初期ユーザー段階）:** 技術的挑戦を優先。Azure AI全サービスを統合する経験は今しか積めない。マネタイズは検証後。
- **ユーザーが付いた段階:** ユーザー価値を最優先。「技術的に面白い」がユーザー不在では意味がない。
- **スケール段階:** お金（コスト最適化）が重要に。$35/月が$350/月になったら構成見直しが必要。

本プロジェクトのコスト構造（$35〜50/月）は、個人開発として持続可能なラインを意識的に設計しています。

---

### Q43. 5年後、どんなエンジニアになっていたいですか

**「設計の意図を語れるアーキテクト」になりたいです。**

本プロジェクトで得た最大の資産は、コードそのものではなく、794行のARCHITECTURE.mdや47件の分析レビュー文書に凝縮された**設計判断の言語化能力**です。

5年後には:
- チームの技術選定で「なぜこのアーキテクチャなのか」をステークホルダーに説明できる
- トレードオフを定量的に評価し（コスト、レイテンシ、開発速度）、最適解を提示できる
- ジュニアメンバーに「なぜ」を教えられる（ARCHITECTURE.mdのように）

---

### Q44. 個人開発は「評価のため」ですか？「探究」ですか？

**明確に「探究」です。ただし、探究の副産物として評価されることは歓迎します。**

このプロジェクトは「AIボイスレコーダーをソフトウェアで作れるか？」という技術的好奇心から始まりました。118件のPRと47件の設計文書は、面接対策のために書いたものではなく、「次の自分が迷わないための記録」として書いたものです。

評価のためだけなら、ここまでやる必要はありません。10言語対応翻訳、FSMリファクタリング、セグメント差分翻訳、SDK統合——これらは「もっと良くできるはず」という探究心の結果です。

---

## 補足：面接官が見ているポイントへの自己評価

| ポイント | 自己評価 |
|---------|---------|
| **具体 → 抽象を行き来できるか** | ✅ コードレベルの実装詳細（SDK APIのメソッド名まで）から、アーキテクチャレベルの設計判断まで一貫して説明できる |
| **「何を作ったか」より「なぜ・どう考えたか」** | ✅ 47件の分析レビュー・実装計画書に思考過程を文書化。技術選定の代替案と不採用理由も説明可能 |
| **成功談より失敗からの学び** | ✅ Boolean爆発→FSM、全文翻訳→差分翻訳、認証障害→自動監視。すべて失敗から学んだ改善 |
| **技術力 × 思考力 × 人間性** | 技術力:✅（Azureフルスタック実運用）、思考力:✅（段階的問題分解）、人間性:△（チーム開発経験を積む必要あり） |
